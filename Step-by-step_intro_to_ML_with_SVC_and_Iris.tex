\documentclass [oneside,10pt,a4paper,ngerman,BCOR10mm,headsepline,parindent,final]{scrartcl}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    % \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    % \usepackage{caption}
    % \DeclareCaptionFormat{nocaption}{}
    % \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \usepackage[utf8]{inputenc}
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    
    % Using fancy headers and footers
    \usepackage{fancyhdr}
    
    % Used for entering author names and their affiliations
    \usepackage[affil-it]{authblk}
    
    % Use bibliography% and configure it
    \usepackage[babel,german=quotes]{csquotes}
    \usepackage[backend=biber,style=authoryear,backref=true]{biblatex}
    \bibliography{literature/notebook.bib}
    \usepackage{url}                    %Output of nicely formatted Internet addresses
    \setcounter{biburllcpenalty}{7000}  %Setting for counter to wrap URLs in literature references
    \setcounter{biburlucpenalty}{8000}  %ditto



    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{\textbf{\textsf{Getting started with Machine Learning (ML) and Support Vector Classifiers (SVC) - A systematic step-by-step approach}}}\author{Dipl.-Ing. Bj\"orn Kasper (\href{mailto:kasper.bjoern@bgetem.de}{kasper.bjoern@bgetem.de})}\affil{Test and Certification Body for Electrical Engineering at BG ETEM}

\date{\today} 


    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy

    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      bookmarksnumbered=true,
      pdfauthor=Dipl.-Ing. Bj\"orn Kasper,
      pdftitle=Getting started with Machine Learning (ML) and Support Vector Classifiers (SVC) - A systematic step-by-step approach,
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      pdfpagemode={UseOutlines},
      pdfview = {XYZ},
      pdfstartview = {XYZ},
      pdfstartpage = {1},
      pdfborder={0 0 0}
      }
    % Slightly bigger margins than the latex defaults
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}



\begin{document}
    
    % Without changing the numbering style,
    % page numbers and column titles should be turned off.
    \pagestyle{empty}
    
    \maketitle\thispagestyle{empty}\begin{center}
        \includegraphics[width=0.90\textwidth]{images/Cover_image.pdf}
        \end{center}
        \vfill

    \begin{abstract}
    Anyone who wants to seriously deal with the emerging topic of our time ``Artificial Intelligence (AI)'' cannot avoid dealing with the basic mathematical models and algorithms from the field of ``Machine Learning (ML)'' as a subset of AI. However, someone who opens the door for the first time to this equally very exciting as well as arbitrarily complex and, at first glance, confusing world will very quickly be overwhelmed. Here, it is a good idea to consult introductory and systematic tutorials. Therefore, this Getting Started tutorial systematically demonstrates the typical ML work process step-by-step using the very powerful and performant ``Support Vector Classifier (SVC)'' and the widely known and exceptionally beginner-friendly ``Iris Dataset''. Furthermore, the selection of the ``correct'' SVC kernel and its parameters are described and their effects on the classification result are shown.
    \end{abstract}
    \vfill
    
    \noindent
    \begin{tabular}{l l}
    \begin{minipage}{0.24\textwidth}
        \includegraphics{images/CC_BY-SA_40.png}
    \end{minipage}
    &
    \begin{minipage}{0.68\textwidth}
        This work is licensed under a \href{https://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)}.
    \end{minipage}
    \end{tabular}

    \newpage

    % Activate own page style
    \pagestyle{fancy}
    % Delete all fields
    \fancyhf{}
    % \fancyhead[EL,OL]{$header$}
    % \fancyfoot[EL,OL]{$footer$}
    % Header leftside: chapter/section
    \fancyhead[ER,OR]{\leftmark}
    % Footer rightside: page number
    \fancyfoot[ER,OR]{Seite \thepage}

    \renewcommand{\sectionmark}[1]{
        \markboth{\thesection{} #1}{}
    }

    
    \tableofcontents
    
    


    
    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

    \hypertarget{english-introduction}{%
\subsection{English introduction}\label{english-introduction}}

In the \textbf{digitized work environment}, there is an increasing
demand for \textbf{Work equipment} to be able to adapt independently and
in a task-related manner to changing work situations. Depending on the
strength of the degree of flexibility, this \textbf{situational
adaptivity} can often only be realized by applying mathematical models
and algorithms from the field of \textbf{Machine Learning (ML)} as a
subset of \textbf{Artificial Intelligence (AI)}.

Examples of such AI applications in work environments can range from
comparatively simple \textbf{voice assistance systems} (similar, for
example, to Siri or Alexa from the private sphere) to partially or
\textbf{highly automated systems}. The transition from
\textbf{automation to autonomy} is currently the subject of much
controversy among experts and can be viewed in terms of the transition
of responsibility from humans to technical systems (\cite{Adler_2021};
\cite{Adler_2019}).

By definition, a system is called \textbf{autonomous} only when it can
achieve a given goal \textbf{independently} and adapted to the situation
\textbf{without human control} or detailed \textbf{programming}
(\cite{EFI_autSysteme_2018}; \cite{acatech_2017}).

However, the distinction between the degree of automation and the
autonomy of a technical system is relatively vague and difficult to
define, depending on the technical context and the degree of
abstraction. Crucial for the classification are the degrees of
\textbf{self-determination}, \textbf{independence} as well as the
\textbf{freedom of decision or action} of a technical system towards
\textbf{human intervention} or preprogrammed behavior patterns (vgl.
\cite{Wiki_Autonomie}).

In contrast to highly automated systems, autonomous systems are only
able to act autonomously, solve problems, and learn to constantly
improve in the process through the use of AI algorithms
(\cite{acatech_2017}).

For example, \textbf{driverless transport systems (AGVs)} can navigate
\textbf{autonomously} through larger industrial facilities using
self-learned self-updated maps shared with other AGVs, and avoid
location-changing obstacles by independently finding and optimizing
suitable routes. However, at a higher level of abstraction, new
logistics tasks are given to them by human operators, which is why AGVs
tend to be \textbf{highly automated systems} from a human perspective.

In addition to the many very interesting advantages, e.g.~in terms of
economic efficiency and workload reduction, such highly automated
systems and, depending on the point of view, autonomous subsystems are
characterized by a very high level of technical complexity. This
concerns both their \textbf{operating functions} (e.g.~autonomous
navigation through complex industrial environments with shared use of
the roadways by other human-controlled vehicles) and their
\textbf{safety functions} (e.g.~evaluation of interlinked imaging and
non-imaging safety sensors for monitoring the driving space to avoid
collisions).

Very high requirements are placed on such autonomous systems and the AI
algorithms used for this purpose with regard to \textbf{functional
safety}. However, the requirements for safety evaluability in terms of
\textbf{transparency} (complete understanding of the system) and
\textbf{explainability} of decisions made by AI are currently very
difficult or impossible to achieve, especially when using AI algorithms
from the field of \textbf{deep learning} (\cite{Liggesmeyer_2019}).

Unlike automated systems, the functionality of AI-powered autonomous
systems is not fully programmed out before operational use, but is
created by applying algorithms with learning capabilities to data. This
results in a model that is merely executed by the software at runtime.
Due to its \textbf{inherent complexity}, the resulting model is
generally \textbf{not comprehensible} to humans, which means that the
\textbf{decisions} of an AI system are often \textbf{not transparent}.
Although the requirements for the AI system typically cannot be fully
described, it must still function reliably later at runtime in a very
large application space (\cite{Schneider_2021}). This pushes today's
established methods and techniques of systematic software design and
testing of safety-related software to their limits (cf.~\textbf{V model}
according to \cite{DIN_EN_61508-3_2011-02}).

Furthermore, in terms of their \textbf{recognition rates} and thus the
\textbf{reliability of their decisions}, today's AI algorithms very
often do not meet the functional safety requirements to achieve higher
safety levels, even under the most favorable conditions. For example, a
software-based safety function with a performance level d \((PL_{d})\)
typically required for machines in accordance with ISO 13849-1 may only
fail dangerously with a probability of \(10^{-7} - 10^{-6}\) per hour
during continuous use (see table K.1 in \cite{DIN_EN_ISO_13849-1_2016}).

Compared to traditional, fully programmed software, the relatively low
robustness of data-driven algorithms from the field of deep learning is
another challenge. This can cause \textbf{small changes} in the
function-determining \textbf{training data} to cause \textbf{large and
unpredictable changes} in system behavior under some circumstances.
However, the \textbf{predictability} and \textbf{transparency} of the
system behavior are elementary for a \textbf{safety verification}
(\cite{BAuA_Rechtsgutachten_KI_2021}).

An appropriate assessment or even \textbf{testing} with regard to the
required functional safety according to uniform and ideally standardized
criteria has numerous consequences for the future orientation and
organization of technical \textbf{occupational safety and health (OSH)}
in Germany and in Europe. In addition to the currently still very
difficult safety-related assessability, an important point is that the
previous clear separation between \textbf{placing on the market law}
(see e.g.~Machinery Directive) and \textbf{occupational safety and
health law} (see European Framework Directive for Occupational Safety
and Health and German Ordinance on Occupational Safety and Health) can
no longer be continued in this way. The reason for this is that
\textbf{safety-related properties} will also change, especially of
systems \textbf{continuously learning} at runtime, due to new or
\textbf{adapted behaviors} learned during operation
(\cite{BAuA_Rechtsgutachten_KI_2021}). From today's point of view,
systems based on \textbf{learned-out} and at runtime \textbf{invariable
models} are not affected by this.

For these reasons, especially the actors of \textbf{technical
occupational safety and health} who will deal with the
\textbf{evaluation} of such \textbf{systems capable of learning} or
system components with AI algorithms in the future should familiarize
themselves in depth with the software structures used for this purpose
as early as possible. This is the only way to ensure that the rapid
development of systems capable of learning can be accompanied by OSH and
their testing authorities in a constructive, critical and technically
appropriate manner. If this is omitted, it must be assumed on the basis
of the experiences of recent years that the OSH system will be
ruthlessly circumvented or undermined by the economic interests of
globally operating software giants. This would have the consequence that
serious or fatal \textbf{occupational accidents} are more likely to
occur \textbf{due to inadequately designed AI-based work systems}.

However, the safety-related evaluation of such learning-capable systems
requires a more in-depth technical entry into the world of
\textbf{machine learning} as a subfield of \textbf{artificial
intelligence}. For this purpose, it is necessary to deal with the basic
operation of typical ML algorithms, corresponding software tools,
libraries and programming systems.

However, someone who opens the door for the first time to this equally
very exciting as well as arbitrarily complex and, at first glance,
confusing world will very quickly be overwhelmed. In addition to reading
general technical literature, it is advisable to consult introductory
and systematic tutorials.

This Getting Started tutorial has exactly this goal, demonstrating
systematically and step-by-step the typical ML workflow using the very
powerful \textbf{Support Vector Classifier (SVC)} as an example.

This tutorial will be presented in the context of a workshop at the
\textbf{Conference ``Artificial Intelligence''}, hosted by the German
Social Accident Insurance (DGUV), probably in November 2022 in Dresden.
The workshop addresses interested ML novices in the technical
occupational safety and health of the social accident insurance
institutions.

Besides the \textbf{deep neural networks}, which are very present in the
media, there is a very rich diversity of other very powerful ML
algorithms - suitable for the particular use case. For a more generally
comprehensible introduction, the SVC algorithm was deliberately chosen
for the target audience of the workshop. Its operating principles are
easy to convey to ML novices as well as in the time frame given for the
workshop - quite in contrast to the entry into the world of deep neural
networks.

The following main sections will demonstrate the typical ML workflow
step-by-step. In \textbf{step 0}, specific guidance is provided for
selecting hardware and software suitable for machine learning. To allow
an ML novice to first familiarize themselves with the ML algorithms,
tools, libraries, and programming systems, the ready-made and very
beginner-friendly \textbf{Iris dataset} is involved in \textbf{step 1}.
Only after a comprehensive acquaintance with the application of ML tools
would it make sense to examine one's own environment for ML-suitable
applications and to obtain suitable datasets from them. However, this is
beyond the scope of this introductory tutorial.

One of the most important steps in the entire ML process is \textbf{step
2}, in which the dataset included in step 1 is examined using typical
data analysis tools. In addition to exploring the \textbf{data
structure} and \textbf{internal correlations} in the dataset, errors
such as gaps, duplications, or obvious misentries must also be found and
corrected where possible. This is enormously important so that the
classification can later provide plausible results.

After exploring the dataset, in \textbf{step 3} one has to decide on a
specific ML algorithm based on certain selection criteria. Among other
ML algorithms suitable for the Iris dataset (such as the
decision-tree-based \textbf{random-forests classifier}), the reasoned
choice here in the tutorial falls on the \textbf{support vector
classifier}. A dedicated SVC model is now being implemented.

In \textbf{step 4} the dataset is prepared for the actual classification
by SVC. Depending on the selected ML algorithm as well as the data
structure, it may be necessary to prepare the data before training
(e.g., by standardization, normalization, or binarization based on
thresholds). After splitting the dataset into a training and test
dataset, the SVC model is trained with the training dataset in
\textbf{step 5}. Subsequently, classification predictions are made with
the trained SVC model based on the test data. In \textbf{step 6}, the
quality of the classification result is evaluated using known
\textbf{metrics} such as the \textbf{confusion matrix}.

Since the classification in step 5 was initially performed with standard
parameters (so-called \textbf{hyper-parameters}), their meaning is
explained in \textbf{step 7} and then their effect on the classification
result is demonstrated by manually varying the individual
hyper-parameters.

In the final \textbf{step 8}, two approaches to systematic
hyper-parameter search are presented: \textbf{Grid Search} and
\textbf{Randomized Search}. While the former exhaustively considers all
parameter combinations for given values, the latter selects a number of
candidates from a parameter space with a particular random distribution.

    \hypertarget{german-introduction}{%
\subsection{German introduction}\label{german-introduction}}

Von den \textbf{Arbeitsmitteln} in der \textbf{digitalisierten
Arbeitswelt} wird immer stärker gefordert, dass sie sich selbstständig
und aufgabenbezogen an sich ändernde Arbeitssituationen anpassen können.
Diese \textbf{situative Adaptivität} kann je nach Stärke des
Flexibilisierungsgrades oft nur durch die Anwendung mathematischer
Modelle und Algorithmen aus dem Bereich des \textbf{Maschinellen Lernens
(ML)} als Teilmenge der \textbf{Künstlichen Intelligenz (KI)} realisiert
werden.

Beispiele für solche KI-Anwendungen in der Arbeitswelt reichen von
vergleichsweise einfachen \textbf{Sprachassistenzsystemen} (ähnlich z.
B. Siri oder Alexa aus dem privaten Umfeld) bis hin zu teil- oder
\textbf{hochautomatisierten Systemen}. Der Übergang von
\textbf{Automatisierung zu Autonomie} wird derzeit in der Fachwelt sehr
kontrovers diskutiert und kann unter dem Aspekt des Übergangs der
Verantwortung vom Menschen zum technischen System betrachtet werden
(\cite{Adler_2021}; \cite{Adler_2019}).

Definitionsgemäß wird ein System erst dann als \textbf{autonom}
bezeichnet, wenn es \textbf{ohne menschliche Steuerung} oder
detaillierte \textbf{Programmierung} ein vorgegebenes Ziel
\textbf{selbstständig} und an die Situation angepasst erreichen kann
(\cite{EFI_autSysteme_2018}; \cite{acatech_2017}).

Allerdings ist die Unterscheidung des Grades der Automatisierung bis hin
zur Autonomie eines technischen Systems relativ fließend und je nach
fachlichem Kontext und Abstraktionsgrad nur schwer zu definieren.
Maßgeblich für die Einordnung sind die Grade der
\textbf{Selbstbestimmtheit}, die \textbf{Unabhängigkeit} sowie die
\textbf{Entscheidungs- bzw. Handlungsfreiheit} eines technischen Systems
gegenüber \textbf{menschlichem Eingriff} oder vorprogrammierter
Verhaltensmuster (vgl. \cite{Wiki_Autonomie}).

Im Gegensatz zu hochautomatisierten Systemen sind autonome Systeme nur
durch Einsatz von KI-Algorithmen in der Lage, eigenständig zu agieren,
Probleme zu lösen und dabei zu lernen, sich ständig zu verbessern
(\cite{acatech_2017}).

Beispielsweise können \textbf{fahrerlose Transportsysteme (FTS)} anhand
selbst erlernter, selbstständig aktualisierter und mit anderen FTS
geteilter Karten \textbf{autonom} durch größere Industrieanlagen
navigieren und ortsveränderlichen Hindernissen ausweichen, indem sie
selbstständig geeignete Routen finden und optimieren. Jedoch werden
ihnen in einer höheren Abstraktionsebene neue Logistikaufträge durch
menschliche Bediener vorgegeben, weswegen es sich bei FTS aus
menschlicher Perspektive eher um \textbf{hochautomatisierte Systeme}
handelt.

Neben den vielen sehr interessanten Vorteilen z. B. bzgl.
Wirtschaftlichkeit und Arbeitserleichterung kennzeichnet solche
hochautomatisierten und je nach Betrachtung autonomen Teilsysteme eine
sehr hohe technische Komplexität. Diese betrifft sowohl ihre
\textbf{Betriebsfunktionen} (z. B. autonome Navigation durch komplexe
industrielle Umgebungen bei gemeinsamer Nutzung der Fahrwege durch
andere menschlich gesteuerte Fahrzeuge) als auch ihre
\textbf{Sicherheitsfunktionen} (z. B. Auswertung miteinander verknüpfter
bildgebender und nicht-bildgebender Sicherheitssensorik zur Überwachung
des Fahrraums zur Kollisionsvermeidung).

An solche autonomen Systeme und die hierfür eingesetzten KI-Algorithmen
werden sehr hohe Anforderungen hinsichtlich der \textbf{funktionalen
Sicherheit} gestellt. Jedoch sind die Anforderungen für eine
sicherheitstechnische Bewertbarkeit bezüglich der \textbf{Transparenz}
(vollständiges Systemverständnis) und \textbf{Erklärbarkeit} der durch
KI getroffenen Entscheidungen insbesondere bei Einsatz von
KI-Algorithmen aus dem Bereich des \textbf{Deep Learnings} derzeit nur
sehr schwer oder gar nicht erreichbar (\cite{Liggesmeyer_2019}).

Im Gegensatz zu automatisierten Systemen wird die Funktionalität
KI-gestützter autonomer Systeme nicht vor der betrieblichen Verwendung
vollständig ausprogrammiert, sondern durch das Anwenden lernfähiger
Algorithmen auf Daten erstellt. Dadurch entsteht ein Modell, das von der
Software zur Laufzeit lediglich ausgeführt wird. Das resultierende
Modell ist aufgrund seiner \textbf{inhärenten Komplexität} im
Allgemeinen \textbf{für den Menschen nicht verständlich}, wodurch die
\textbf{Entscheidungen} eines KI-Systems oft \textbf{nicht transparent}
sind. Obwohl die Anforderungen an das KI-System typischerweise nicht
vollständig beschrieben werden können, muss es später zur Laufzeit in
einem sehr großen Anwendungsraum trotzdem verlässlich funktionieren
(\cite{Schneider_2021}). Dadurch kommen die heute etablierten Methoden
und Techniken des systematischen Softwareentwurfes und -testens
sicherheitsgerichteter Software an ihre Grenzen (vgl. \textbf{V-Modell}
nach \cite{DIN_EN_61508-3_2011-02}).

Weiterhin erfüllen heutige KI-Algorithmen hinsichtlich ihrer
erreichbaren \textbf{Erkennungsraten} und damit der
\textbf{Zuverlässigkeiten ihrer Entscheidungen} selbst unter günstigsten
Bedingungen sehr oft nicht die Anforderungen an die funktionale
Sicherheit, um höhere Safety-Level zu erreichen. Beispielsweise darf
eine software-gestützte Sicherheitsfunktion mit einem für Maschinen
typischerweise geforderten Performance Level d \((PL_{d})\) nach ISO
13849-1 bei kontinuierlicher Nutzung nur mit einer Wahrscheinlichkeit
von \(10^{-7} - 10^{-6}\) pro Stunde gefährlich ausfallen (siehe Tabelle
K.1 in \cite{DIN_EN_ISO_13849-1_2016}).

Im Vergleich zu traditioneller, vollständig ausprogrammierter Software
ist bei datengetriebenen Algorithmen aus dem Bereich des Deep Learnings
die verhältnismäßig geringe Robustheit eine weitere Herausforderung.
Diese kann dazu führen, dass \textbf{kleine Änderungen} in den
funktionsbestimmenden \textbf{Trainingsdaten} unter Umständen
\textbf{große und unvorhersehbare Veränderungen} des Systemverhaltens
bewirken. Jedoch sind die \textbf{Vorhersehbarkeit} und
\textbf{Nachvollziehbarkeit} des Systemverhaltens für einen
\textbf{Sicherheitsnachweis} elementar
(\cite{BAuA_Rechtsgutachten_KI_2021}).

Eine hinsichtlich der geforderten funktionalen Sicherheit angemessene
Bewertung oder gar \textbf{Prüfung} nach einheitlichen und idealerweise
genormten Maßstäben hat viele Konsequenzen für die zukünftige
Ausrichtung und Gestaltung des \textbf{technischen Arbeitsschutzes} in
Deutschland und in Europa. Neben der derzeit noch sehr schwierigen
sicherheitstechnischen Bewertbarkeit von KI-Algorithmen ist ein
wichtiger Punkt, dass die bisherige klare Trennung zwischen
\textbf{Inverkehrbringensrecht} (siehe z. B. Maschinenrichtlinie) und
\textbf{betrieblichem Arbeitsschutzrecht} (siehe
Arbeitsschutz-Rahmenrichtlinie und Betriebssicherheitsverordnung) so
nicht mehr aufrechterhalten werden kann. Grund hierfür ist, dass sich
auch die \textbf{sicherheitsrelevanten Eigenschaften} insbesondere von
zur Laufzeit \textbf{weiterlernenden Systemen} durch während des
Betriebs erlernte, neue oder \textbf{angepasste Verhaltensweisen}
verändern werden (\cite{BAuA_Rechtsgutachten_KI_2021}). Systeme auf
Basis \textbf{ausgelernter} und zur Laufzeit \textbf{unveränderlicher
Modelle} sind aus heutiger Sicht hiervon nicht betroffen.

Aus diesen Gründen sollten sich insbesondere die Akteure des
\textbf{technischen Arbeitsschutzes}, die sich zukünftig mit der
\textbf{Prüfung} solcher \textbf{lernfähigen Systeme} oder
Systemkomponenten mit KI-Algorithmen befassen werden, möglichst
frühzeitig mit den hierfür eingesetzten Software-Strukturen vertieft
auseinandersetzen. Nur dadurch lässt sich erreichen, dass die stürmische
Entwicklung lernfähiger Systeme durch den Arbeitsschutz und dessen
Prüfinstitute konstruktiv, kritisch und fachlich angemessen begleitet
werden kann. Wird dies versäumt, muss aufgrund der Erfahrungen der
vergangenen Jahre davon ausgegangen werden, dass das Arbeitsschutzsystem
durch die wirtschaftlichen Interessen global agierender Softwaregiganten
skrupellos umgangen oder ausgehebelt werden wird. Dies hätte die Folge,
dass schwere oder tödliche \textbf{Arbeitsunfälle wegen unzulänglich
gestalteter KI-basierter Arbeitssysteme} wahrscheinlicher werden.

Allerdings erfordert die sicherheitstechnische Bewertung solcher
lernfähigen Systeme einen tiefer gehenden fachlichen Einstieg in die
Welt des \textbf{maschinellen Lernens} als Teilgebiet der
\textbf{künstlichen Intelligenz}. Hierzu muss sich mit den grundlegenden
Funktionsweisen typischer ML-Algorithmen, entsprechenden
Software-Werkzeugen, Bibliotheken und Programmiersystemen auseinander
gesetzt werden.

Wer jedoch zum ersten Mal die Tür zu dieser ebenso spannenden wie
beliebig komplexen und auf den ersten Blick verwirrenden Welt öffnet,
wird sehr schnell überfordert sein. Hier empfiehlt es sich neben dem
Lesen allgemeiner Fachliteratur, einführende und systematische
Anleitungen zu Rate zu ziehen.

Genau dieses Ziel verfolgt das vorliegende Getting-Started-Tutorial,
indem systematisch und Schritt-für-Schritt der typische ML-Arbeitsablauf
am Beispiel des sehr leistungsfähigen \textbf{Support Vector Classifier
(SVC)} demonstriert wird.

Dieses Tutorial wird im Rahmen eines Workshops auf der
\textbf{Fachtagung ``Künstliche Intelligenz''}, ausgerichtet durch die
Deutsche Gesetzliche Unfallversicherung (DGUV), voraussichtlich im
November 2022 in Dresden vorgestellt. Der Workshop richtet sich an
interessierte ML-Neulinge im technischen Arbeitsschutz der gesetzlichen
Unfallversicherungsträger.

Neben den medial sehr präsenten \textbf{tiefen neuronalen Netzen} gibt
es eine sehr reichhaltige Auswahl anderer sehr leistungsfähiger
ML-Algorithmen - passend für den jeweiligen Anwendungsfall. Für einen
allgemein verständlicheren Einstieg wurde für die Zielgruppe des
Workshops der SVC-Algorithmus bewusst gewählt. Dessen Arbeitsweise ist
sowohl für ML-Neulinge als auch in dem für den Workshop vorgegebenen
Zeitrahmen leicht vermittelbar - ganz im Gegensatz zum Einstieg in die
Welt der tiefen neuronalen Netze.

Die folgenden Hauptabschnitte demonstrieren den typischen
ML-Arbeitsablauf Schritt-für-Schritt. Im \textbf{Schritt 0} werden
konkrete Hinweise für die Auswahl der für das maschinelle Lernen
geeigneten Hardware und Software gegeben. Damit sich ein ML-Neuling
zunächst mit den ML-Algorithmen, Werkzeugen, Bibliotheken und
Programmiersystemen vertraut machen kann, wird im \textbf{Schritt 1} der
fertige und sehr einsteigerfreundliche \textbf{Iris-Datensatz}
hinzugezogen. Erst nach einer umfassenden Einarbeitung in die Anwendung
der ML-Werkzeuge wäre es sinnvoll, die eigene Umgebung auf ML-taugliche
Anwendungen hin zu untersuchen und daraus geeignete Datensätze zu
gewinnen. Dies geht jedoch über den Rahmen dieses einführenden Tutorials
hinaus.

Mit der wichtigste Schritt im gesamten ML-Prozess ist \textbf{Schritt
2}, in dem der in Schritt 1 einbezogene Datensatz mit Hilfe typischer
Datenanalyse-Werkzeuge untersucht wird. Neben der Erkundung der
\textbf{Datenstruktur} sowie \textbf{innerer Zusammenhänge} im Datensatz
müssen auch Fehler wie z. B. Lücken, Dopplungen oder offensichtliche
Fehleingaben gefunden und nach Möglichkeit behoben werden. Dies ist
enorm wichtig, damit die Klassifikation später plausible Ergebnisse
liefern kann.

Nach der Erkundung des Datensatzes muss man sich im \textbf{Schritt 3}
anhand bestimmter Auswahlkriterien für einen konkreten ML-Algorithmus
entscheiden. Neben anderen für den Iris-Datensatz passenden
ML-Algorithmen (wie z. B. der entscheidungsbaum-basierte
\textbf{Random-forests-Classifier}) fällt die begründete Auswahl hier im
Tutorial auf den \textbf{Support-Vector-Classifier}. Ein entsprechendes
SVC-Modell wird nun implementiert.

Im \textbf{Schritt 4} wird der Datensatz für die eigentliche
Klassifikation per SVC vorbereitet. Je nach gewähltem ML-Algorithmus
sowie der Datenstruktur kann es erforderlich sein, dass die Daten vor
dem Training aufbereitet werden müssen (z. B. durch Standardisierung,
Normalisierung oder Binärisierung anhand von Schwellwerten). Nach der
Aufteilung des Datensatzes in einen Trainings- und Testdatensatz, wird
das SVC-Modell im \textbf{Schritt 5} mit dem Trainingsdatensatz
trainiert. Anschließend werden mit dem trainierten SVC-Modell anhand der
Testdaten Klassifikationsvorhersagen getroffen. Im \textbf{Schritt 6}
wird die Güte des Klassifikationsergebnisses anhand bekannter
\textbf{Metriken} wie z. B. der \textbf{Konfusionsmatrix} evaluiert.

Da die Klassifikation im Schritt 5 zunächst mit Standard-Parametern (den
sogenannte \textbf{Hyper-Parametern}) durchgeführt wurde, wird ihre
Bedeutung im \textbf{Schritt 7} erklärt und danach ihr Einfluss auf das
Klassifikationsergebnis durch manuelle Variation der einzelnen
Hyper-Parameter demonstriert.

Im abschließenden \textbf{Schritt 8} werden zwei Ansätze zur
systematischen Hyper-Parameter-Suche vorgestellt: \textbf{Grid Search}
und \textbf{Randomized Search}. Während bei ersterer für gegebene Werte
erschöpfend alle Parameterkombinationen betrachtet werden, wird beim
zweiten Ansatz eine Anzahl von Kandidaten aus einem Parameterraum mit
einer bestimmten zufälligen Verteilung ausgewählt.

    \hypertarget{steps-of-the-systematic-ml-process}{%
\subsection{Steps of the systematic ML
process}\label{steps-of-the-systematic-ml-process}}

The following \textbf{steps of the systematic ML process} are covered in
the next main sections:

\begin{itemize}
\tightlist
\item
  \hyperref[step-0-select-hardware-and-software-suitable-for-ml]{STEP 0: Select hardware and software suitable for ML}
\item
  \hyperref[step-1-acquire-the-ml-dataset]{STEP 1: Acquire the ML dataset}
\item
  \hyperref[step-2-explore-the-ml-dataset]{STEP 2: Explore the ML dataset}
\item
  \hyperref[step-3-choose-and-create-the-ml-model]{STEP 3: Choose and create the ML model}
\item
  \hyperref[step-4-prepare-the-dataset-for-training]{STEP 4: Prepare the dataset for training}
\item
  \hyperref[step-5-carry-out-training-prediction-and-testing]{STEP 5: Carry out training, prediction and testing}
\item
  \hyperref[step-6-evaluate-models-performance]{STEP 6: Evaluate model’s performance}
\item
  \hyperref[step-7-vary-parameters-of-the-ml-model-manually]{STEP 7: Vary parameters of the ML model manually}
\item
  \hyperref[step-8-tune-the-ml-model-systematically]{STEP 8: Tune the ML model systematically}
\end{itemize}

    \hypertarget{step-0-select-hardware-and-software-suitable-for-ml}{%
\section{STEP 0: Select hardware and software suitable for
ML}\label{step-0-select-hardware-and-software-suitable-for-ml}}

In this step, specific guidance is provided for selecting hardware and
software suitable for machine learning.

    \hypertarget{community-support}{%
\subsection{Community Support}\label{community-support}}

When selecting and deciding for or against the use of certain hardware
and software components, in addition to purely technical or financial
characteristics, significant attention should be paid to broad
\textbf{support from a well-networked community}. This community should
consist of a balanced share of \textbf{manufacturers} of hardware
components (e.g.~GPU suppliers, manufacturers of embedded systems or
sensors), \textbf{software developers} ideally from the \textbf{open
source} ecosystem, and an active \textbf{user community} (e.g.~for
reporting hardware and software bugs or providing help in forums).

The author's many years of development experience show that the
technically best hardware or software component is worthless if you are
(apparently) the only user. This impression arises either because the
component is actually very exotic and has only a few users or because
the development takes place ``behind closed doors'', i.e.~in the
company's internal \textbf{closed source} domain.

Without the support of an active community, you are (almost) on your own
when it comes to questions or problems. Progress in the development and
maintenance of an AI application is therefore very difficult! The clear
recommendation is therefore: Go for the (technically, price-wise, etc.)
\textbf{second-best alternative} but with an even bigger
\textbf{community}.

    \hypertarget{hardware}{%
\subsection{Hardware}\label{hardware}}

When considering hardware requirements, two systems and their use cases
must be taken into account: the \textbf{training system} and the
\textbf{application system}.

\hypertarget{training-system}{%
\subsubsection{Training system}\label{training-system}}

The \textbf{training phase} requires a lot of \textbf{computational
power} and \textbf{memory (RAM)}, depending on the \textbf{amount of
data} to be processed and the \textbf{ML algorithm (so-called
estimator)} chosen.

Depending on the estimator model, highly parallel processing on a
\textbf{Graphics Processing Unit (GPU)} can provide significant
\textbf{speed advantages} over processing on a \textbf{Central
Processing Unit (CPU)} (e.g., when training deep neural networks in the
area of \textbf{deep learning}). To take advantage of this speed
benefit, the AI application must be suitable in terms of
\textbf{parallelizability} of the estimator model used as well as
\textbf{GPU support} through special driver layers, the so-called
\href{https://en.wikipedia.org/wiki/Operating_system_abstraction_layer}{Operating
System Abstraction Layer (OSAL)} (\cite{Wiki_OSAL}).

Such GPUs are installed on powerful \textbf{3D graphics cards}. However,
these must be explicitly qualified for the application for AI - not
every game-suitable graphics card from any manufacturer can be used. The
manufacturer \textbf{Nvidia} offers GPUs suitable for AI in its
high-performance graphics cards with \textbf{CUDA architecture}.
\href{https://en.wikipedia.org/wiki/CUDA}{CUDA} stands for ``Compute
Unified Device Architecture'' and is a \textbf{programming interface}
(API) developed by Nvidia, with which program parts can be processed by
the graphics processor (\cite{Wiki_CUDA}). A GPU with its several tens
of thousands of threads can process highly parallelizable tasks that
require only little data communication between the memory areas
significantly more performantly than conventional CPUs. This speed
advantage can be considerable despite currently available CPU
technologies like \textbf{Multicore} with \textbf{Hyper-Threading} with
Intel CPUs!

Nvidia graphics cards with CUDA-supporting GPUs are ranked based on
their \textbf{\href{https://developer.nvidia.com/cuda-gpus}{compute
capability}} (\cite{NVIDIA_CUDA_CAP_2022}).

However, it should be mentioned that currently only the manufacturer
Nvidia offers 3D graphics cards with CUDA implementation, since CUDA is
a \textbf{proprietary} framework. In addition, there is also the much
less well-known \textbf{open source} alternative
\textbf{\href{https://en.wikipedia.org/wiki/OpenCL}{OpenCL}}, which has
now been implemented by a large number of graphics card manufacturers
(\cite{Wiki_OpenCL}). Since OpenCL is an \textbf{open industry
standard}, Intel and AMD chips and their GPUs, ATI Radeon cards of the
5, 6, 7 and R9 series as well as various Nvidia GeForce cards are
supported, for example.

Regarding the \textbf{code execution performance} of both alternatives
in direct comparison, there are different statements in the technical
literature. The 2011 paper \href{https://arxiv.org/abs/1005.2581}{A
Performance Comparison of CUDA and OpenCL} sees the CUDA implementation
as the clear favorite (\cite{CUDA_OpenCL_Perf_2011}). More recent
publications point out the strong dependence of performance on
\textbf{code quality}, \textbf{algorithm type} and the \textbf{GPU
hardware} used, among other things - see e.g.~here:
\href{https://www.incredibuild.com/blog/cuda-vs-opencl-which-to-use-for-gpu-programming}{CUDA
vs OpenCL: Which to Use for GPU Programming}
(\cite{CUDA_vs_OpenCL_2021}).

It is therefore recommended that the decision for \textbf{CUDA or
OpenCL} should depend on the extent to which most of the applications
employed and the GPU hardware used are better supported by one of the
two approaches in each case.

The \textbf{state of the art} should be also taken into account when
selecting the rest of the training system's hardware. Otherwise,
seemingly (price-wise) inexpensive components could very quickly nullify
the speed advantage of the GPU. In addition to a mainboard suitable for
one (or more) high-performance graphics cards with a correspondingly
powerful BUS system (e.g.~PCI Express), the RAM should be as large as
possible (min. 64 GB) and fast. A large RAM allows, for example, the
\textbf{virtualization} of several parallel systems in the form of
\textbf{\href{https://en.wikipedia.org/wiki/Virtual_machine}{virtual
machines}} and thus a significantly better utilization of the available
computing capacity (\cite{Wiki_VM}). The permanent memory should also be
as large and fast as possible - high-performance \textbf{solid-state
drives (SSDs)} should be clearly preferred over classic hard disks
(HDDs).

    \hypertarget{application-system}{%
\subsubsection{Application system}\label{application-system}}

In the \textbf{application phase} of the trained estimator model,
considerably less computing power and RAM are usually required. If the
concrete application does not require \textbf{continuous learning during
operation}, significantly less expensive systems (in terms of
acquisition costs, power consumption, etc.) can also be used. Such
application-specific \textbf{embedded systems} have only one CPU
(usually in \textbf{ARM architecture}), comparatively limited RAM
(e.g.~1 - 8 GB) and usually no GPU. A popular \textbf{embedded computer}
that is very well supported in terms of ML software is the
\textbf{\href{https://en.wikipedia.org/wiki/Raspberry_Pi}{Raspberry Pi}}
(\cite{Wiki_Raspi}). In addition to its ARM CPU, the Raspberry Pi also
has a GPU installed on the same processor in the so-called
\textbf{System on a Chip design (SoC)}. However, the SoC manufacturer
\textbf{Broadcom} does not support the CUDA API.

There are references in the technical literature that the open source
alternative \textbf{OpenCL} can be installed on the Raspberry Pi and
that the AI framework \textbf{TensorFLow} (see section ``Software'') can
be compiled with
\textbf{\href{https://en.wikipedia.org/wiki/SYCL}{SYCL}} support, where
SYCL stands for ``Single Source OpenCL'' (\cite{Wiki_SYCL}). However, a
first rough review gives the impression that support for this approach
is still very experimental at the moment. Therefore, parallelizing the
AI application on the GPU of the Raspberry Pi does not seem to be an
option (yet). Here are some links for further reading:

\begin{itemize}
\tightlist
\item
  \href{https://qengineering.eu/deep-learning-with-raspberry-pi-and-alternatives.html}{Deep
  learning with Raspberry Pi and alternatives in 2022}
  (\cite{DL_Raspi_2022})
\item
  \href{https://www.hackster.io/news/benchmarking-machine-learning-on-the-new-raspberry-pi-4-model-b-88db9304ce4}{Benchmarking
  Machine Learning on the New Raspberry Pi 4, Model B}
  (\cite{ML_Raspi4_2019})
\item
  \href{https://towardsdatascience.com/portable-computer-vision-tensorflow-2-0-on-a-raspberry-pi-part-1-of-2-84e318798ce9}{Portable
  Computer Vision: TensorFlow 2.0 on a Raspberry Pi}
  (\cite{TF2_Raspi4_2019})
\item
  \href{https://qengineering.eu/install-opencl-on-raspberry-pi-3.html}{Install
  OpenCL on Raspberry Pi 3 B+} (\cite{OpenCL_Raspi_2022})
\item
  \href{https://indiantechwarrior.com/does-tensorflow-support-opencl/}{Does
  TensorFlow Support OpenCL?} (\cite{TF_OpenCL_2022})
\item
  \href{https://www.codeplay.com/portal/blogs/2016/06/01/tensorflow-for-opencl-using-sycl.html}{TensorFlow
  for OpenCL using SYCL} (\cite{TF_OpenCL_SYCL_2016})
\end{itemize}

    \hypertarget{software}{%
\subsection{Software}\label{software}}

    \hypertarget{programming-languages}{%
\subsubsection{Programming languages}\label{programming-languages}}

The comparison of \textbf{advantages and disadvantages} of the various
programming languages and the evaluation of their suitability for ML was
inspired by the following articles, among others:

\begin{itemize}
\tightlist
\item
  \href{https://www.springboard.com/blog/data-science/best-language-for-machine-learning/}{What
  Is the Best Language for Machine Learning?}
  (\cite{ML_bestLanguage_2021})
\item
  \href{https://datasciencenerd.com/is-octave-good-for-machine-learning/}{Is
  Octave Good for Machine Learning?} (\cite{Octave_for_ML_2021})
\end{itemize}

In summary, there is \textbf{no best language for machine learning},
each is good where it fits best.

However, there are definitely some programming languages that are better
suited for machine learning tasks than others
(\cite{ML_bestLanguage_2021}). On the one hand, this is due to whether
the programming language is fundamentally well suited to
\textbf{implement complex mathematical and statistical tasks} in
efficient algorithms.

On the other hand, when deciding for or against a programming language,
it should definitely also be taken into account whether it contains
sufficient \textbf{basic functionalities for data analysis and its
processing}, as well as very diverse \textbf{extension libraries}
(so-called \textbf{packages}) that are well supported by the community
are available. By using these libraries, it is possible to concentrate
on the concrete task when creating an ML application and not have to
constantly solve the same trivial problems anew in every new application
(e.g.~the efficient \textbf{handling of datasets} or the execution of
\textbf{matrices calculations}).

Following trend chart shows how the
\href{https://insights.stackoverflow.com/trends?tags=python\%2Cr\%2Coctave\%2Cjava\%2Cc\%2B\%2B}{popularity
of selected programming languages} suitable for machine learning has
evolved since 2008:

\begin{figure}
\centering
\includegraphics{images/2022-09-07_StackOverflowTrends_ProgrammingLanguages_wide.png}
\caption{Trend chart shows popularity of programming languages for ML
(source:
\href{https://insights.stackoverflow.com/trends?tags=python\%2Cr\%2Coctave\%2Cjava\%2Cc\%2B\%2B}{Stack
Overflow Trends}, license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{python}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/Python_(programming_language)}{Python}}{Python}}\label{python}}

It is a high-level, \textbf{general-purpose} programming language where
its design philosophy emphasizes \textbf{code readability}. The
\textbf{variable types} in Python are \textbf{dynamic} and
\textbf{memory} is \textbf{automatically managed} to create and delete
data objects (see
\href{https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)}{garbage
collection}).

\textbf{Pros:}

\begin{itemize}
\tightlist
\item
  Python offers simple, concise, and \textbf{readable code} for allowing
  to write robust and reliable programs.
\item
  It lets you focus on solving the ML problem instead of getting lost in
  the language's technical nuances.
\item
  Python has \textbf{extensive libraries for ML},
  e.g.~\texttt{Scikit-learn}, \texttt{Pandas}, \texttt{TensorFlow} or
  \texttt{Keras} have become standard libraries for various ML tasks.
\item
  The language has been around for decades and has developed a large and
  helpful community.
\item
  Besides extensive online documentation, there are thousands of
  question-answers and community guides for various functionalities of
  the language (this is also very well reflected in the trend graph on
  the popularity of programming languages).
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\tightlist
\item
  (unknown drawbacks so far \ldots)
\end{itemize}

    \hypertarget{r}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/R_(programming_language)}{R}}{R}}\label{r}}

It is a programming language for \textbf{statistical computing} and
\textbf{graphics} supported by the \textbf{R Core Team} and the
\textbf{R Foundation for Statistical Computing}. Created by
statisticians Ross Ihaka and Robert Gentleman, R is used among data
miners, bioinformaticians and statisticians for data analysis and
developing statistical software.

\textbf{Pros:}

\begin{itemize}
\tightlist
\item
  After Python, R is the recommended ML programming language.
\item
  R is a flexible and cross-platform compatible language.
\item
  It has a growing, supportive community.
\item
  R is well suited for data visualization and \textbf{statistics}, often
  making it the language of choice for applications with a large amount
  of statistical data.
\item
  It is considered a powerful choice for \textbf{machine learning},
  offering a variety of machine learning techniques (e.g., data
  visualization, data sampling, data analysis and supervised and
  unsupervised machine learning models) via post-installable libraries.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\tightlist
\item
  R is often reported to be laggier and slower as compared to Python
  when dealing with large-scale datasets.
\item
  It has a \textbf{significantly lower community} support when answering
  questions or giving guidance \textbf{compared to Python} (see trend
  chart on popularity of programming languages).
\item
  The \textbf{learning curve} for the basic entry into R and the
  application in more complex projects for data analysis or machine
  learning is significantly \textbf{steeper than with Python}.
\end{itemize}

    \hypertarget{java}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/Java_(programming_language)}{Java}}{Java}}\label{java}}

It is a high-level, \textbf{class-based}, \textbf{object-oriented}
programming language that is designed to have \textbf{as few
implementation dependencies} as possible. It is a
\textbf{general-purpose} programming language intended that compiled
Java code can run on all platforms that support Java without the need to
recompile.

\textbf{Pros:}

\begin{itemize}
\tightlist
\item
  Using Java for machine learning is especially popular among developers
  with a Java background, as it skips the need to learn another
  programming language such as Python or R.
\item
  Like Python and R, Java also has a variety of third-party machine
  learning libraries, e.g.~\textbf{JavaML} is a built-in library with a
  collection of algorithms implemented in Java for ML.
\item
  Scalability is an important feature for many ML projects, which is
  well supported by Java.
\item
  \href{https://en.wikipedia.org/wiki/Java_virtual_machine}{Java Virtual
  Machine (JVM)} enables the development of ML applications for multiple
  platforms.
\item
  Java is very well suited for speed-critical ML projects.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\tightlist
\item
  Java has a much lower community support in answering questions or
  giving guidance compared to Python - but a better one than R (see
  trend chart on popularity of programming languages).
\end{itemize}

    \hypertarget{gnu-octave}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/GNU_Octave}{GNU
Octave}}{GNU Octave}}\label{gnu-octave}}

It is a high-level programming language that's \textbf{designed for
numerical computations} (\cite{Octave_for_ML_2021}).

\textbf{Pros:}

\begin{itemize}
\tightlist
\item
  With Octave, \textbf{linear and non-linear numerical problems} can be
  solved quickly.
\item
  Octave is syntactically very similar to
  \href{https://en.wikipedia.org/wiki/MATLAB}{MATLAB} and mostly
  \textbf{compatible with MATLAB}. If no MATLAB-specific functions are
  used, the program code also runs in Octave. In addition, Octave even
  has some language functions and a syntax diversity that MATLAB lacks.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\tightlist
\item
  However, Octave is not a good programming language for machine
  learning in a production environment.
\item
  It doesn't have the same functionality as other languages used for ML,
  due to \textbf{missing libraries} and frameworks to speed up ML tasks.
\item
  It's not as flexible, simple, and feature-rich as other programming
  languages.
\item
  Compared to Python, R and Java, Octave has almost \textbf{no community
  support} when it comes to answering questions or providing guidance
  (compare trend chart on popularity of programming languages).
\end{itemize}

    \hypertarget{python-packages}{%
\subsubsection{Python packages}\label{python-packages}}

The \textbf{mathematics} and the \textbf{numerical implementation} of
various algorithms for data analysis and machine learning are usually
\textbf{very complex} and often only comprehensible for ML experts
(\cite{ML_bestLanguage_2021}). For a broad and praxis-oriented
\textbf{usability}, better \textbf{reusability of code} and a successful
\textbf{integration} into a concrete ML application, the functional
relationships should be \textbf{encapsulated in libraries} (so-called
``packages'').

From the user's point of view, when selecting libraries for the
respective task, attention should be paid not only to functionality but
also to the \textbf{comprehensibility of the user interface supported by
good documentation}. Furthermore, the \textbf{size of the community}
behind the library, consisting of active developers as well as technical
experts for supporting the users in the event of questions or problems
arising, should be decisive in the selection.

Following trend chart shows how the
\href{https://insights.stackoverflow.com/trends?tags=pandas\%2Ctensorflow\%2Cnumpy\%2Ckeras\%2Cscikit-learn\%2Cmatplotlib}{popularity
of selected python packages} suitable for \textbf{data analysis},
\textbf{data visualization} and \textbf{machine learning} has evolved
since 2008:

\begin{figure}
\centering
\includegraphics{images/2022-09-07_StackOverflowTrends_MLPythonPackages_wide.png}
\caption{Trend chart shows popularity of selected python packages for
data analysis, data visualization and machine learning (source:
\href{https://insights.stackoverflow.com/trends?tags=pandas\%2Ctensorflow\%2Cnumpy\%2Ckeras\%2Cscikit-learn\%2Cmatplotlib}{Stack
Overflow Trends}, license: CC BY-SA 4.0)}
\end{figure}

In the scientific research and systematic improvement of ML algorithms,
a very dynamic progression can be observed in recent years. The latest
scientific findings are regularly compared with each other in
\textbf{``Machine Learning Competitions''} using known and
\textbf{freely available datasets} (see benchmarking competitions of ML
algorithms on platforms such as
\url{https://www.kaggle.com/competitions}). At the same time, the
corresponding ML libraries are revised, extended and made available to
general users by the scientific community. Therefore, this
\textbf{scientific transfer} ideally takes place in the context of
\textbf{open source developments}.

Due to the superior advantages of \textbf{Python} (see previous
section), a selection of \textbf{open source} packages available for
this programming language usable for \textbf{data analysis},
\textbf{data visualization} and \textbf{machine learning} are presented
in this section.

    \hypertarget{data-analysis}{%
\paragraph{Data analysis}\label{data-analysis}}

\begin{itemize}
\item
  \href{https://numpy.org/devdocs/user/whatisnumpy.html}{NumPy} is a
  Python library that provides a \textbf{multidimensional array object},
  various derived objects (such as masked arrays and \textbf{matrices}),
  and an assortment of routines for \textbf{fast operations on arrays},
  including mathematical, logical, shape manipulation, sorting,
  selecting, discrete Fourier transformations, basic linear algebra,
  basic statistical operations, random simulation and much more.
\item
  \href{https://pandas.pydata.org/docs/getting_started/overview.html}{Pandas}
  is a Python package providing fast, flexible and expressive data
  structures designed to work with \textbf{relational} or
  \textbf{labeled} datasets. It provides two primary data structures:
  \texttt{pandas.Series} (1-dimensional time series) and
  \texttt{pandas.DataFrame} (2-dimensional spreadsheets). The data
  structure \texttt{pandas.DataFrame} offers the same functionality as
  the structure \texttt{data.frame} known from the programming language
  R and much more.
\end{itemize}

    \hypertarget{data-visualization}{%
\paragraph{Data visualization}\label{data-visualization}}

\begin{itemize}
\item
  \href{https://matplotlib.org}{Matplotlib} is a library for making
  \textbf{2D plots of arrays} in Python. Although it has its origins in
  \textbf{emulating the MATLAB graphics commands}, it is independent of
  MATLAB, and can be used in a Pythonic, object oriented way. Although
  Matplotlib is written primarily in pure Python, it makes heavy use of
  NumPy and other extension code to provide good performance even for
  large arrays (\cite{Hunter_matplotlib_2007}).
\item
  \href{https://seaborn.pydata.org/}{Seaborn} is a library for making
  \textbf{statistical graphics} in Python. It builds \textbf{on top of
  matplotlib} and integrates closely with \textbf{pandas data
  structures}. Seaborn helps to explore and understand the data. Its
  plotting functions operate on \textbf{dataframes} and \textbf{arrays}
  containing whole datasets and internally perform the necessary
  semantic mapping and statistical aggregation to produce informative
  plots (\cite{Waskom_seaborn_2021}).
\end{itemize}

    \hypertarget{machine-learning}{%
\paragraph{Machine learning}\label{machine-learning}}

\begin{itemize}
\item
  \href{https://scikit-learn.org/stable/}{Scikit-Learn} is a
  \textbf{free software machine learning library} for \textbf{Python}.
  It features various \textbf{classification}, \textbf{regression} and
  \textbf{clustering} algorithms including \textbf{support-vector
  machines}, \textbf{random forests}, \textbf{gradient boosting} and
  \textbf{k-means}. It is designed to interoperate with the Python
  numerical and scientific libraries \textbf{NumPy} and \textbf{SciPy}.
  Scikit-Learn will be used in the next steps of this of this getting
  started tutorial.
\item
  \href{https://www.tensorflow.org}{TensorFlow} offers, among other
  things, the possibility to create and train \textbf{artificial neural
  networks (ANN)} based on \textbf{Google AI}. it is an open source
  software library for \textbf{machine learning} and \textbf{artificial
  intelligence}. It can be used across a range of tasks but has a
  particular focus on training and inference of \textbf{deep neural
  networks}. However, the installation and usage is very much beyond the
  scope of this beginner tutorial.
\item
  \href{https://keras.io/about/}{Keras} is an open source software
  library for \textbf{deep learning} that provides a Python interface
  for \textbf{ANNs}. Keras acts as an \textbf{general interface} for
  several \textbf{backends}, such as \textbf{TensorFlow},
  \textbf{Microsoft Cognitive Toolkit} and \textbf{Theano}. Keras will
  also not be used in this beginner tutorial.
\end{itemize}

    \hypertarget{import-python-packages-globally}{%
\subsubsection{Import Python packages
globally}\label{import-python-packages-globally}}

The aim of this section is to import globally used Python packages for
data analysis and ML, such as \texttt{Pandas}, \texttt{NumPY},
\texttt{matplotlib} and \texttt{Scikit-Learn}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{time}

\PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k+kn}{import} \PY{n}{HTML}

\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{svm}\PY{p}{,} \PY{n}{metrics}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{programming-ides}{%
\subsubsection{Programming IDEs}\label{programming-ides}}

\textbf{\href{https://en.wikipedia.org/wiki/Integrated_development_environment}{Integrated
development environments (IDE)}} are software applications that provide
comprehensive features to computer programmers for \textbf{software
development}. An IDE typically consists of a \textbf{source code
editor}, automated \textbf{build tools} for compiling or an
\textbf{interpreter} for scripting languages, a front end to the
\textbf{version control system} like
e.g.~\href{https://en.wikipedia.org/wiki/Git}{Git} and a
\textbf{debugger} (\cite{Wiki_IDE}).

Following trend chart shows how the
\href{https://insights.stackoverflow.com/trends?tags=rstudio\%2Cjupyter-notebook\%2Cvisual-studio-code}{popularity
of selected IDEs} suitable for ML programming languages has evolved
since 2008:

\begin{figure}
\centering
\includegraphics{images/2022-09-07_StackOverflowTrends_IDEs_wide.png}
\caption{Trend chart shows popularity of selected IDEs for ML
programming languages (source:
\href{https://insights.stackoverflow.com/trends?tags=rstudio\%2Cjupyter-notebook\%2Cvisual-studio-code}{Stack
Overflow Trends}, license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{visual-studio-code-vsc}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/Visual_Studio_Code}{Visual
Studio Code
(VSC)}}{Visual Studio Code (VSC)}}\label{visual-studio-code-vsc}}

It is an IDE made by \textbf{Microsoft} for \textbf{Windows},
\textbf{Linux} and \textbf{macOS}. Features include support for
\textbf{debugging}, \textbf{syntax highlighting} for many different
programming languages, intelligent \textbf{code completion} and embedded
\textbf{version control system} Git. Users can change the theme,
keyboard shortcuts, preferences, and install \textbf{extensions} from a
huge repository that add additional functionality. Despite of its
platform independence, VSC is \textbf{not open source} - in fact it is
released under a traditional
\href{https://code.visualstudio.com/License/}{Microsoft product
license}.

\begin{figure}
\centering
\includegraphics{images/Screenshot_VSC.png}
\caption{Screenshot of IDE \emph{Visual Studio Code} (source: Kasper,
license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{jupyterlab}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/Project_Jupyter\#Jupyter_Notebook}{JupyterLab}}{JupyterLab}}\label{jupyterlab}}

It is the successor product for the web-based interactive environment
\textbf{Jupyter Notebook}. Within this IDE, Jupyter Notebook documents
can be created, edited, and executed interactively. The notebooks
consist of \textbf{input and output cells}, each of which can contain
program code, formatted text in \textbf{Markdown} format and live plots
generated from the code.

Jupyter is a new \textbf{open source} alternative to the proprietary
numerical software
\href{https://en.wikipedia.org/wiki/Wolfram_Mathematica}{Mathematica}
from \textbf{Wolfram Research} that is well on the way to becoming a
\textbf{standard for exchanging research results}
(\cite{Scientific_Paper_obsolete_2018};
\cite{Future_of_Research_Paper_2018}).

Originally Jupyter was intended as an IDE for the programming languages
\textbf{Julia} and \textbf{Python}. Besides that it is also possible to
install other interpreter kernels, such as the
\textbf{\href{https://irkernel.github.io/installation/}{IRkernel}} for
R. This can be interesting if the IDE \textbf{RStudio Desktop} is not
available on the target platform used. For example, it is very difficult
to install RStudio on the ARM-based embedded computer \textbf{Raspberry
Pi} due to many technical dependencies. In contrast, using the R kernel
in JupyterLab on the Raspberry Pi works very well and performant.

\begin{figure}
\centering
\includegraphics{images/Screenshot_JupyterLab.png}
\caption{Screenshot of IDE \emph{JupyterLab} (source: Kasper, license:
CC BY-SA 4.0)}
\end{figure}

    \hypertarget{rstudio}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/RStudio}{RStudio}}{RStudio}}\label{rstudio}}

It is an IDE and graphical user interface for the statistical
programming language \textbf{R} offered by \textbf{RStudio, Inc.} and is
made available in two formats. \textbf{RStudio Desktop} is a regular
desktop application while \textbf{RStudio Server} runs on a remote
server and allows accessing RStudio using a web browser. Both software
products are available in \textbf{open source} and \textbf{commercial}
versions, each with different functionalities.

The program editor in RStudio allows \textbf{autocompletion},
\textbf{automatic indentation}, \textbf{syntax highlighting},
\textbf{code folding} as well as \textbf{integrated help} and
information about functions and objects in the working environment.
There is the ability to view and edit the contents of variables and
datasets. To facilitate collaboration, scripts, data and other files can
be combined into projects (.Rproj) and versioned with \textbf{Git}.

\begin{figure}
\centering
\includegraphics{images/Screenshot_RStudio.png}
\caption{Screenshot of IDE \emph{RStudio} (source: Kasper, license: CC
BY-SA 4.0)}
\end{figure}

    \hypertarget{gnu-octave-gui}{%
\paragraph{\texorpdfstring{\href{https://en.wikipedia.org/wiki/GNU_Octave\#User_interfaces}{GNU
Octave (GUI)}}{GNU Octave (GUI)}}\label{gnu-octave-gui}}

It is the official graphical user interface for the \textbf{GNU Octave}
programming language and is available for Windows, macOS, Linux and BSD
under \textbf{Open Source} licensing.

If the command line interpreter (CLI) starts instead of the graphical
user interface (GUI) when \texttt{octave} is called, this can be forced
via the \texttt{octave\ -\/-gui} option.

\begin{figure}
\centering
\includegraphics{images/Screenshot_GNU_Octave.png}
\caption{Screenshot of IDE \emph{GNU Octave} (source: Kasper, license:
CC BY-SA 4.0)}
\end{figure}

    \hypertarget{cloud-hosted-ides}{%
\subsubsection{Cloud-hosted IDEs}\label{cloud-hosted-ides}}

A very interesting alternative to own, local and for the ML application
adequately powerful and thus price-intensive hardware resources can be
\textbf{cloud-hosted Jupyter environments}. These offer features such as
cloud storage, model training and deployment capabilities, version
control and much more.

Since the entire hardware and backend configurations are hosted in the
cloud by the various providers, the user can concentrate on creating his
ML application. The cloud provider takes care of purchasing the hardware
and the sometimes time-consuming installation and configuration of the
programming environment (\cite{Colab_Alternatives_2021}).

The cloud environments briefly presented here can be used freely after
registration - on condition that own projects remain accessible to other
researchers. Even in the free variant, GPUs and
\href{https://en.wikipedia.org/wiki/Tensor_Processing_Unit}{Tensor
Processing Units (TPUs)} can be selected in the project for hardware
acceleration. This is particularly interesting for training deep neural
networks.

In the premium versions, for example, more powerful GPUs and TPUs as
well as more memory can be accessed. Additionally, there is the option
to keep the projects private and thus prevent accessibility for other
researchers.

However, with all the advantages, \textbf{data protection aspects}
should definitely be considered. Before using a cloud environment, it
should be clarified whether and to what extent, for example, \textbf{own
datasets with personal data} may be uploaded to the cloud projects. If
there are uncertainties here, local and self-hosted ML resources should
be used in any case!

    \hypertarget{google-colaboratory}{%
\paragraph{\texorpdfstring{\href{https://colab.research.google.com/}{Google
Colaboratory}}{Google Colaboratory}}\label{google-colaboratory}}

In recent years, \textbf{Google Colaboratory} (\textbf{Colab} for short)
has become a popular choice for cloud-based Jupyter notebooks. Thanks to
its free-to-use GPUs and cloud storage linked to Google Drive, it is
used by many researchers in the ML and data science community
(\cite{Colab_Alternatives_2021}).

Due to the similarity of the web interface to Jupyter, Python developers
can write and run arbitrary Python program codes. Colab is a
cloud-hosted version of Jupyter Notebook that provides free access to
compute infrastructure such as memory, storage, processing capacity,
GPUs and TPUs (\cite{Colab_about_2022}).

Furthermore, commonly used libraries such as \textbf{PyTorch},
\textbf{TensorFlow} and \textbf{Keras} can be used to develop deep
learning applications (\cite{Colab_5_Alternatives_2021}).

\begin{figure}
\centering
\includegraphics{images/Screenshot_google_Colab.png}
\caption{Screenshot of IDE \emph{Google Colaboratory} (source: Kasper,
license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{google-kaggle}{%
\paragraph{\texorpdfstring{\href{https://www.kaggle.com}{Google
Kaggle}}{Google Kaggle}}\label{google-kaggle}}

This is another Google product with similar functionality to Colab. Like
Colab, \textbf{Kaggle} also offers free browser-based Jupyter notebooks
and the use of GPUs. Kaggle also has many \textbf{Python packages
pre-installed}, which lowers the barrier to entry for many users
(\cite{Colab_Alternatives_2021}).

Kaggle and Colab have a number of similarities - among other things,
most of the keyboard shortcuts are the same as in Jupyter notebooks.
Furthermore, many datasets can be imported. Kaggle has a large user
community to learn and improve data science skills
(\cite{Colab_5_Alternatives_2021}).

\begin{figure}
\centering
\includegraphics{images/Screenshot_google_Kaggle.png}
\caption{Screenshot of IDE \emph{Google Kaggle} (source: Kasper,
license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{paperspace-gradient}{%
\paragraph{\texorpdfstring{\href{https://www.paperspace.com/gradient/notebooks}{Paperspace
Gradient}}{Paperspace Gradient}}\label{paperspace-gradient}}

Unlike Colab, \textbf{Paperspace Gradient} can implement entire
\textbf{ML workflows} from data pre-processing to training models to
deploying the trained models. Furthermore, Gradient has features like a
CLI tool, more control over the GPU and simpler data management
services. Due to the variety of functions, one must first become
familiar with the operation of the significantly more complex user
interface (\cite{Free_GPUs_for_ML_2020}).

\begin{figure}
\centering
\includegraphics{images/Screenshot_Paperspace_Gradient.png}
\caption{Screenshot of IDE \emph{Paperspace Gradient} (source: Kasper,
license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{operating-systems}{%
\subsubsection{Operating systems}\label{operating-systems}}

The \textbf{programming languages}, \textbf{Python libraries} and
\textbf{development environments} presented in the previous sections are
available for different operating systems, such as \textbf{Linux},
\textbf{Windows} and \textbf{macOS}. Therefore, the decision for or
against an operating system may \textbf{depend on the technical
background} of the ML developer.

Nevertheless, the following general \textbf{requirements} can be
specified for an operating system \textbf{suitable for software
development}:

\begin{itemize}
\tightlist
\item
  \textbf{Openness}: availability of very good interface documentation
  and ideally open source software
\item
  \textbf{Self-administration}: user has full installation and
  configuration rights
\item
  \textbf{Communication capability}: unfiltered and bidirectional
  communication in the local network as well as to the internet on all
  necessary protocols possible
\item
  \textbf{Extensibility}:

  \begin{itemize}
  \tightlist
  \item
    automated software installation and update management via central
    package management systems such as \texttt{apt}, \texttt{pip} or
    \texttt{conda}
  \item
    possible integration of additional software libraries or external
    sensor hardware
  \end{itemize}
\end{itemize}

Following trend chart shows how the
\href{https://insights.stackoverflow.com/trends?tags=windows\%2Clinux\%2Cmacos}{popularity
of selected operating systems} used by \textbf{data analysts} and
\textbf{ML developers} has evolved since 2008:

\begin{figure}
\centering
\includegraphics{images/2022-09-07_StackOverflowTrends_OperatingSystems_wide.png}
\caption{Trend chart shows popularity of selected operating systems used
by \textbf{data analysts} and \textbf{ML developers} (source:
\href{https://insights.stackoverflow.com/trends?tags=windows\%2Clinux\%2Cmacos}{Stack
Overflow Trends}, license: CC BY-SA 4.0)}
\end{figure}

For \textbf{security} reasons, the \textbf{IT departments} of many
employers massively \textbf{restrict installation and configuration
rights}. Furthermore, very restrictive firewall settings severely
\textbf{restrict} unfiltered and bidirectional \textbf{communication} in
the local network and to the internet. Automated \textbf{software
installations} via package managers are often \textbf{not possible} or
only possible with difficulty due to blocked protocols.

To deal with these challenges, two possible solutions are presented
below.

    \hypertarget{virtual-machine}{%
\paragraph{Virtual machine}\label{virtual-machine}}

To be able to install, configure and update the required software (IDEs,
programming languages and ML packages) independently, the use of a
\href{https://en.wikipedia.org/wiki/Virtual_machine}{Virtual Machine
(VM)} could be a possible alternative.

However, there are also significant disadvantages here:

\begin{itemize}
\tightlist
\item
  The \textbf{communication problem} is \textbf{not solved}, because the
  VM shares the access to the internet with the host system.
\item
  The \textbf{access to 3D graphics cards} is usually \textbf{not
  possible} due to virtualization.
\item
  This solution has only \textbf{low application performance}, as
  regular business computers are often only very sparsely equipped in
  terms of RAM and processor performance for cost reasons.
\end{itemize}

    \hypertarget{separate-lab-computer}{%
\paragraph{Separate lab computer}\label{separate-lab-computer}}

All the problems mentioned in the previous section can only be solved
satisfactorily by acquiring a \textbf{separate laboratory computer} with
\textbf{its own internet access} (e.g.~via an \textbf{LTE-capable wifi
router}).

This laboratory computer can be configured according to your own
requirements, depending on the available budget in terms of hardware and
software.

However, it should be noted here that the \textbf{IT departments} of
many employers do \textbf{not offer any support} for this solution. You
are usually responsible for software installation, maintenance and
troubleshooting yourself!

    \hypertarget{step-1-acquire-the-ml-dataset}{%
\section{STEP 1: Acquire the ML
dataset}\label{step-1-acquire-the-ml-dataset}}

To allow an ML novice to first familiarize themselves with the ML
algorithms, tools, libraries, and programming systems, the ready-made
and very beginner-friendly \textbf{Iris dataset} is involved in this
step. Only after a comprehensive acquaintance with the application of ML
tools would it make sense to examine one's own environment for
ML-suitable applications and to obtain suitable datasets from them.
However, this is beyond the scope of this introductory tutorial.

Several details, for example, on the history of the creation of the
\href{https://en.wikipedia.org/wiki/Iris_flower_data_set}{Iris flower
dataset} can be found e.g.~on Wikipedia (see \cite{Wiki_IrisDS}).

It can be downloaded on
\href{https://www.kaggle.com/datasets/arshid/iris-flower-dataset}{Kaggle:
Iris Flower Dataset} (\cite{Kaggle_IrisDS}). Furthermore, the dataset is
available via Python in the machine learning package
\href{https://scikit-learn.org}{Scikit-learn}, so that users can access
it without having to find a special source for it.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import Iris dataset for exploration}
\PY{n}{irisdata\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./datasets/IRIS\PYZus{}flower\PYZus{}dataset\PYZus{}kaggle.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{step-2-explore-the-ml-dataset}{%
\section{STEP 2: Explore the ML
dataset}\label{step-2-explore-the-ml-dataset}}

One of the most important steps in the entire ML process is this step,
in which the dataset included in Step 1 is examined using typical data
analysis tools. In addition to exploring the \textbf{data structure} and
\textbf{internal correlations} in the dataset, errors such as
\textbf{gaps}, \textbf{duplications}, or obvious \textbf{misentries}
must also be found and corrected where possible. This is enormously
important so that the classification can later provide plausible
results.

\hypertarget{goals-of-exploration}{%
\subsection{Goals of exploration}\label{goals-of-exploration}}

The objectives of the exploration of the dataset are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clarify the \textbf{origins history}:

  \begin{itemize}
  \tightlist
  \item
    Where did the data come from? \textbf{→} Contact persons and
    licensing permissions?
  \item
    Who obtained the data and with which (measurement) methods?
    \textbf{→} Did systematic errors occur during the acquisition?
  \item
    What were they originally intended for? \textbf{→} Can they be used
    for my application?
  \end{itemize}
\item
  Overview of the internal \textbf{structure and organisation} of the
  data:

  \begin{itemize}
  \tightlist
  \item
    Which columns are there? \textbf{→} With which methods can they be
    read in (e.g.~import of CSV files)?
  \item
    What do they contain for (physical) measured variables? \textbf{→}
    Which technical or physical correlations exist?
  \item
    Which data formats or types are there? \textbf{→} Do they have to be
    converted?
  \item
    In which value ranges do the measurement data vary? \textbf{→} Are
    normalizations necessary?
  \end{itemize}
\item
  Identify \textbf{anomalies} in the datasets:

  \begin{itemize}
  \tightlist
  \item
    Do the data have \textbf{gaps} or \textbf{duplicates}? \textbf{→}
    Does the dataset needs to be cleaned?
  \item
    Are there obvious erroneous entries or measurement outliers?
    \textbf{→} Does (statistical) filtering have to be carried out?
  \end{itemize}
\item
  Avoidance of \textbf{tendencies due to bias}:

  \begin{itemize}
  \tightlist
  \item
    Are all possible classes included in the dataset and equally
    distributed? \textbf{→} Does the dataset need to be enriched with
    additional data for balance?
  \end{itemize}
\item
  Find a first rough \textbf{idea of which correlations} could be in the
  dataset
\end{enumerate}

    \hypertarget{clarify-the-origins-history}{%
\subsection{\texorpdfstring{Clarify the \textbf{origins
history}}{Clarify the origins history}}\label{clarify-the-origins-history}}

\begin{quote}
The
\textbf{\href{https://en.wikipedia.org/wiki/Iris_flower_data_set}{Iris
flower datasets}} is a multivariate dataset introduced by the British
statistician and biologist \emph{Ronald Fisher} in his paper
``\href{https://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x}{The
use of multiple measurements in taxonomic problems}''
(\cite{Fisher_1936}). It is sometimes called \emph{Anderson's Iris
dataset} because Edgar Anderson collected the data to quantify the
morphologic variation of Iris flowers of three related species
(\cite{Wiki_IrisDS}).
\end{quote}

The dataset is published in Public Domain with a
\href{https://creativecommons.org/share-your-work/public-domain/cc0/}{CC0-License}.

This dataset became a typical test case for many statistical
classification techniques in machine learning such as \textbf{support
vector machines}.

\begin{quote}
{[}..{]} measurements of the flowers of fifty plants each of the two
species \emph{Iris setosa} and \emph{I. versicolor}, found
\textbf{growing together in the same colony} and measured by Dr E.
Anderson (\cite{Fisher_1936})
\end{quote}

\begin{quote}
{[}..{]} \emph{Iris virginica}, differs from the two other samples in
\textbf{not being taken from the same natural colony} (ibidem)
\end{quote}

    \hypertarget{overview-of-the-internal-structure-and-organization-of-the-data}{%
\subsection{\texorpdfstring{Overview of the internal \textbf{structure
and organization} of the
data}{Overview of the internal structure and organization of the data}}\label{overview-of-the-internal-structure-and-organization-of-the-data}}

The dataset consists of 50 samples from each of three species of Iris:
\href{https://en.wikipedia.org/wiki/Iris_setosa}{\emph{Iris setosa}},
\href{https://en.wikipedia.org/wiki/Iris_virginica}{\emph{Iris
virginica}} and
\href{https://en.wikipedia.org/wiki/Iris_versicolor}{\emph{Iris
versicolor}}, so there are 150 samples in total
(\cite{Wiki_Iris_setosa}, \cite{Wiki_Iris_virginica} and
\cite{Wiki_Iris_versicolor}).

Four features were measured from each sample: the length and the width
of the \textbf{\href{https://en.wikipedia.org/wiki/Sepal}{sepals}} and
\textbf{\href{https://en.wikipedia.org/wiki/Petal}{petals}}, in
centimetres (\cite{Wiki_Sepal} and \cite{Wiki_Petal}). Here you can see
a principle illustration of a flower in which, among other things, the
sepals and petals are shown:

    \begin{figure}
\centering
\includegraphics{images/Mature_flower_diagram_1024px.png}
\caption{Principle illustration of a flower with sepal and petal
(source:
\href{https://en.wikipedia.org/wiki/File:Mature_flower_diagram.svg}{Mature\_flower\_diagram.svg},
license: public domain)}
\end{figure}

    Here are pictures of the three different Iris species (\emph{Iris
setosa}, \emph{Iris virginica} and \emph{Iris versicolor}). Given the
dimensions of the flower, it will be possible to predict the class of
the flower.

    \begin{figure}
\centering
\includegraphics{images/Iris_images.png}
\caption{Left: \emph{Iris setosa} (source:
\href{https://commons.wikimedia.org/wiki/File:Irissetosa1.jpg}{Irissetosa1.jpg},
license: public domain); middle: \emph{Iris versicolor} (source:
\href{https://en.wikipedia.org/wiki/File:Iris_versicolor_3.jpg}{Iris\_versicolor\_3.jpg},
license: CC SA 3.0); right: \emph{Iris virginica} (source:
\href{https://en.wikipedia.org/wiki/File:Iris_virginica.jpg}{Iris\_virginica.jpg},
license: CC SA 2.0)}
\end{figure}

    \hypertarget{inspect-structure-of-dataframe}{%
\subsubsection{\texorpdfstring{Inspect \textbf{structure of
dataframe}}{Inspect structure of dataframe}}\label{inspect-structure-of-dataframe}}

Print first or last 10 rows of dataframe:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   sepal\_length  sepal\_width  petal\_length  petal\_width      species
0           5.1          3.5           1.4          0.2  Iris-setosa
1           4.9          3.0           1.4          0.2  Iris-setosa
2           4.7          3.2           1.3          0.2  Iris-setosa
3           4.6          3.1           1.5          0.2  Iris-setosa
4           5.0          3.6           1.4          0.2  Iris-setosa
5           5.4          3.9           1.7          0.4  Iris-setosa
6           4.6          3.4           1.4          0.3  Iris-setosa
7           5.0          3.4           1.5          0.2  Iris-setosa
8           4.4          2.9           1.4          0.2  Iris-setosa
9           4.9          3.1           1.5          0.1  Iris-setosa
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{tail}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     sepal\_length  sepal\_width  petal\_length  petal\_width         species
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica
\end{Verbatim}
\end{tcolorbox}
        
    While printing a dataframe - only an abbreviated view of the dataframe
is shown :(\\
Default setting in the pandas library makes it to display only 5 lines
from head and from tail.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     sepal\_length  sepal\_width  petal\_length  petal\_width         species
0             5.1          3.5           1.4          0.2     Iris-setosa
1             4.9          3.0           1.4          0.2     Iris-setosa
2             4.7          3.2           1.3          0.2     Iris-setosa
3             4.6          3.1           1.5          0.2     Iris-setosa
4             5.0          3.6           1.4          0.2     Iris-setosa
..            {\ldots}          {\ldots}           {\ldots}          {\ldots}             {\ldots}
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica

[150 rows x 5 columns]
\end{Verbatim}
\end{tcolorbox}
        
    To print all rows of a dataframe, the option \texttt{display.max\_rows}
has to set to \texttt{None} in pandas:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.max\PYZus{}rows}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}
\PY{n}{irisdata\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     sepal\_length  sepal\_width  petal\_length  petal\_width          species
0             5.1          3.5           1.4          0.2      Iris-setosa
1             4.9          3.0           1.4          0.2      Iris-setosa
2             4.7          3.2           1.3          0.2      Iris-setosa
3             4.6          3.1           1.5          0.2      Iris-setosa
4             5.0          3.6           1.4          0.2      Iris-setosa
5             5.4          3.9           1.7          0.4      Iris-setosa
6             4.6          3.4           1.4          0.3      Iris-setosa
7             5.0          3.4           1.5          0.2      Iris-setosa
8             4.4          2.9           1.4          0.2      Iris-setosa
9             4.9          3.1           1.5          0.1      Iris-setosa
10            5.4          3.7           1.5          0.2      Iris-setosa
11            4.8          3.4           1.6          0.2      Iris-setosa
12            4.8          3.0           1.4          0.1      Iris-setosa
13            4.3          3.0           1.1          0.1      Iris-setosa
14            5.8          4.0           1.2          0.2      Iris-setosa
15            5.7          4.4           1.5          0.4      Iris-setosa
16            5.4          3.9           1.3          0.4      Iris-setosa
17            5.1          3.5           1.4          0.3      Iris-setosa
18            5.7          3.8           1.7          0.3      Iris-setosa
19            5.1          3.8           1.5          0.3      Iris-setosa
20            5.4          3.4           1.7          0.2      Iris-setosa
21            5.1          3.7           1.5          0.4      Iris-setosa
22            4.6          3.6           1.0          0.2      Iris-setosa
23            5.1          3.3           1.7          0.5      Iris-setosa
24            4.8          3.4           1.9          0.2      Iris-setosa
25            5.0          3.0           1.6          0.2      Iris-setosa
26            5.0          3.4           1.6          0.4      Iris-setosa
27            5.2          3.5           1.5          0.2      Iris-setosa
28            5.2          3.4           1.4          0.2      Iris-setosa
29            4.7          3.2           1.6          0.2      Iris-setosa
30            4.8          3.1           1.6          0.2      Iris-setosa
31            5.4          3.4           1.5          0.4      Iris-setosa
32            5.2          4.1           1.5          0.1      Iris-setosa
33            5.5          4.2           1.4          0.2      Iris-setosa
34            4.9          3.1           1.5          0.1      Iris-setosa
35            5.0          3.2           1.2          0.2      Iris-setosa
36            5.5          3.5           1.3          0.2      Iris-setosa
37            4.9          3.1           1.5          0.1      Iris-setosa
38            4.4          3.0           1.3          0.2      Iris-setosa
39            5.1          3.4           1.5          0.2      Iris-setosa
40            5.0          3.5           1.3          0.3      Iris-setosa
41            4.5          2.3           1.3          0.3      Iris-setosa
42            4.4          3.2           1.3          0.2      Iris-setosa
43            5.0          3.5           1.6          0.6      Iris-setosa
44            5.1          3.8           1.9          0.4      Iris-setosa
45            4.8          3.0           1.4          0.3      Iris-setosa
46            5.1          3.8           1.6          0.2      Iris-setosa
47            4.6          3.2           1.4          0.2      Iris-setosa
48            5.3          3.7           1.5          0.2      Iris-setosa
49            5.0          3.3           1.4          0.2      Iris-setosa
50            7.0          3.2           4.7          1.4  Iris-versicolor
51            6.4          3.2           4.5          1.5  Iris-versicolor
52            6.9          3.1           4.9          1.5  Iris-versicolor
53            5.5          2.3           4.0          1.3  Iris-versicolor
54            6.5          2.8           4.6          1.5  Iris-versicolor
55            5.7          2.8           4.5          1.3  Iris-versicolor
56            6.3          3.3           4.7          1.6  Iris-versicolor
57            4.9          2.4           3.3          1.0  Iris-versicolor
58            6.6          2.9           4.6          1.3  Iris-versicolor
59            5.2          2.7           3.9          1.4  Iris-versicolor
60            5.0          2.0           3.5          1.0  Iris-versicolor
61            5.9          3.0           4.2          1.5  Iris-versicolor
62            6.0          2.2           4.0          1.0  Iris-versicolor
63            6.1          2.9           4.7          1.4  Iris-versicolor
64            5.6          2.9           3.6          1.3  Iris-versicolor
65            6.7          3.1           4.4          1.4  Iris-versicolor
66            5.6          3.0           4.5          1.5  Iris-versicolor
67            5.8          2.7           4.1          1.0  Iris-versicolor
68            6.2          2.2           4.5          1.5  Iris-versicolor
69            5.6          2.5           3.9          1.1  Iris-versicolor
70            5.9          3.2           4.8          1.8  Iris-versicolor
71            6.1          2.8           4.0          1.3  Iris-versicolor
72            6.3          2.5           4.9          1.5  Iris-versicolor
73            6.1          2.8           4.7          1.2  Iris-versicolor
74            6.4          2.9           4.3          1.3  Iris-versicolor
75            6.6          3.0           4.4          1.4  Iris-versicolor
76            6.8          2.8           4.8          1.4  Iris-versicolor
77            6.7          3.0           5.0          1.7  Iris-versicolor
78            6.0          2.9           4.5          1.5  Iris-versicolor
79            5.7          2.6           3.5          1.0  Iris-versicolor
80            5.5          2.4           3.8          1.1  Iris-versicolor
81            5.5          2.4           3.7          1.0  Iris-versicolor
82            5.8          2.7           3.9          1.2  Iris-versicolor
83            6.0          2.7           5.1          1.6  Iris-versicolor
84            5.4          3.0           4.5          1.5  Iris-versicolor
85            6.0          3.4           4.5          1.6  Iris-versicolor
86            6.7          3.1           4.7          1.5  Iris-versicolor
87            6.3          2.3           4.4          1.3  Iris-versicolor
88            5.6          3.0           4.1          1.3  Iris-versicolor
89            5.5          2.5           4.0          1.3  Iris-versicolor
90            5.5          2.6           4.4          1.2  Iris-versicolor
91            6.1          3.0           4.6          1.4  Iris-versicolor
92            5.8          2.6           4.0          1.2  Iris-versicolor
93            5.0          2.3           3.3          1.0  Iris-versicolor
94            5.6          2.7           4.2          1.3  Iris-versicolor
95            5.7          3.0           4.2          1.2  Iris-versicolor
96            5.7          2.9           4.2          1.3  Iris-versicolor
97            6.2          2.9           4.3          1.3  Iris-versicolor
98            5.1          2.5           3.0          1.1  Iris-versicolor
99            5.7          2.8           4.1          1.3  Iris-versicolor
100           6.3          3.3           6.0          2.5   Iris-virginica
101           5.8          2.7           5.1          1.9   Iris-virginica
102           7.1          3.0           5.9          2.1   Iris-virginica
103           6.3          2.9           5.6          1.8   Iris-virginica
104           6.5          3.0           5.8          2.2   Iris-virginica
105           7.6          3.0           6.6          2.1   Iris-virginica
106           4.9          2.5           4.5          1.7   Iris-virginica
107           7.3          2.9           6.3          1.8   Iris-virginica
108           6.7          2.5           5.8          1.8   Iris-virginica
109           7.2          3.6           6.1          2.5   Iris-virginica
110           6.5          3.2           5.1          2.0   Iris-virginica
111           6.4          2.7           5.3          1.9   Iris-virginica
112           6.8          3.0           5.5          2.1   Iris-virginica
113           5.7          2.5           5.0          2.0   Iris-virginica
114           5.8          2.8           5.1          2.4   Iris-virginica
115           6.4          3.2           5.3          2.3   Iris-virginica
116           6.5          3.0           5.5          1.8   Iris-virginica
117           7.7          3.8           6.7          2.2   Iris-virginica
118           7.7          2.6           6.9          2.3   Iris-virginica
119           6.0          2.2           5.0          1.5   Iris-virginica
120           6.9          3.2           5.7          2.3   Iris-virginica
121           5.6          2.8           4.9          2.0   Iris-virginica
122           7.7          2.8           6.7          2.0   Iris-virginica
123           6.3          2.7           4.9          1.8   Iris-virginica
124           6.7          3.3           5.7          2.1   Iris-virginica
125           7.2          3.2           6.0          1.8   Iris-virginica
126           6.2          2.8           4.8          1.8   Iris-virginica
127           6.1          3.0           4.9          1.8   Iris-virginica
128           6.4          2.8           5.6          2.1   Iris-virginica
129           7.2          3.0           5.8          1.6   Iris-virginica
130           7.4          2.8           6.1          1.9   Iris-virginica
131           7.9          3.8           6.4          2.0   Iris-virginica
132           6.4          2.8           5.6          2.2   Iris-virginica
133           6.3          2.8           5.1          1.5   Iris-virginica
134           6.1          2.6           5.6          1.4   Iris-virginica
135           7.7          3.0           6.1          2.3   Iris-virginica
136           6.3          3.4           5.6          2.4   Iris-virginica
137           6.4          3.1           5.5          1.8   Iris-virginica
138           6.0          3.0           4.8          1.8   Iris-virginica
139           6.9          3.1           5.4          2.1   Iris-virginica
140           6.7          3.1           5.6          2.4   Iris-virginica
141           6.9          3.1           5.1          2.3   Iris-virginica
142           5.8          2.7           5.1          1.9   Iris-virginica
143           6.8          3.2           5.9          2.3   Iris-virginica
144           6.7          3.3           5.7          2.5   Iris-virginica
145           6.7          3.0           5.2          2.3   Iris-virginica
146           6.3          2.5           5.0          1.9   Iris-virginica
147           6.5          3.0           5.2          2.0   Iris-virginica
148           6.2          3.4           5.4          2.3   Iris-virginica
149           5.9          3.0           5.1          1.8   Iris-virginica
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{get-data-types}{%
\subsubsection{Get data types}\label{get-data-types}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
 \#   Column        Non-Null Count  Dtype
---  ------        --------------  -----
 0   sepal\_length  150 non-null    float64
 1   sepal\_width   150 non-null    float64
 2   petal\_length  150 non-null    float64
 3   petal\_width   150 non-null    float64
 4   species       150 non-null    object
dtypes: float64(4), object(1)
memory usage: 5.3+ KB
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
       sepal\_length  sepal\_width  petal\_length  petal\_width
count    150.000000   150.000000    150.000000   150.000000
mean       5.843333     3.054000      3.758667     1.198667
std        0.828066     0.433594      1.764420     0.763161
min        4.300000     2.000000      1.000000     0.100000
25\%        5.100000     2.800000      1.600000     0.300000
50\%        5.800000     3.000000      4.350000     1.300000
75\%        6.400000     3.300000      5.100000     1.800000
max        7.900000     4.400000      6.900000     2.500000
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{get-data-ranges-and-distribution}{%
\subsubsection{Get data ranges and
distribution}\label{get-data-ranges-and-distribution}}

    \hypertarget{histograms}{%
\paragraph{Histograms}\label{histograms}}

\textbf{Histograms} are useful to explore the frequency distribution for
each feature in univariate plots:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{98}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}context}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{notebook}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.3}\PY{p}{,} \PY{n}{rc}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lines.linewidth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mf}{2.0}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{n\PYZus{}bins} \PY{o}{=} \PY{l+m+mi}{10}
\PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{bins} \PY{o}{=} \PY{n}{n\PYZus{}bins}\PY{p}{)}\PY{p}{;}
\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sepal Length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}

\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{bins} \PY{o}{=} \PY{n}{n\PYZus{}bins}\PY{p}{)}\PY{p}{;}
\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sepal Width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}

\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{bins} \PY{o}{=} \PY{n}{n\PYZus{}bins}\PY{p}{)}\PY{p}{;}
\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal Length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}

\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{bins} \PY{o}{=} \PY{n}{n\PYZus{}bins}\PY{p}{)}\PY{p}{;}
\PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal Width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}

\PY{c+c1}{\PYZsh{} add some spacing between subplots}
\PY{n}{fig}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{n}{pad}\PY{o}{=}\PY{l+m+mf}{2.0}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_52_0.png}\end{center}
        \caption{Histograms used to explore the frequency distribution of the 4 features in the Iris dataset}
        \label{fig:histogram_iris}
    \end{figure}
    
    \hypertarget{boxplots}{%
\paragraph{Boxplots}\label{boxplots}}

\textbf{Boxplots} can be used to explore the \textbf{data ranges} in the
dataset. These also provide information about \textbf{outliers}.

In the following code example, the 4 variables of the Iris dataset are
displayed side-by-side in individual boxplots:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{99}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}context}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{notebook}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.3}\PY{p}{,} \PY{n}{rc}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lines.linewidth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mf}{2.0}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}sns.set\PYZus{}style(\PYZdq{}white\PYZdq{})}

\PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}

\PY{n}{cn} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}setosa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}versicolor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}virginica}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} x, y: names of variables in data or vector data}
\PY{c+c1}{\PYZsh{} data: dataset for plotting}
\PY{c+c1}{\PYZsh{} order: order to plot the categorical levels in}
\PY{c+c1}{\PYZsh{} ax: assignment of the plot to the matplotlib subplot}
\PY{n}{box1} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{data} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{box2} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{data} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{box3} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{data} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{box4} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{data} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{,}  \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} add some spacing between subplots}
\PY{n}{fig}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{n}{pad}\PY{o}{=}\PY{l+m+mf}{2.0}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_54_0.png}\end{center}
        \caption{Boxplots used to explore the data ranges in the Iris dataset}
        \label{fig:boxplots_iris}
    \end{figure}
    
    \hypertarget{violin-plots}{%
\paragraph{Violin plots}\label{violin-plots}}

Another type of visualization is the \textbf{violin plot}, which
\textbf{combines} the advantages of both the \textbf{histogram} and the
\textbf{box plot}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{100}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}context}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{notebook}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.3}\PY{p}{,} \PY{n}{rc}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lines.linewidth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mf}{2.0}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}

\PY{n}{cn} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}setosa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}versicolor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}virginica}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{violin1} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{violinplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{data}\PY{o}{=}\PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{violin2} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{violinplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{data}\PY{o}{=}\PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{violin3} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{violinplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{data}\PY{o}{=}\PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n}{violin4} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{violinplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                         \PY{n}{data}\PY{o}{=}\PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{order} \PY{o}{=} \PY{n}{cn}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axs}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} add some spacing between subplots}
\PY{n}{fig}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{n}{pad}\PY{o}{=}\PY{l+m+mf}{2.0}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_56_0.png}\end{center}
        \caption{Violin plots combine histograms and box plots}
        \label{fig:violinplots_iris}
    \end{figure}
    
    \hypertarget{identify-anomalies-in-the-datasets}{%
\subsection{\texorpdfstring{Identify \textbf{anomalies} in the
datasets}{Identify anomalies in the datasets}}\label{identify-anomalies-in-the-datasets}}

\hypertarget{find-and-repair-gaps-in-dataset}{%
\subsubsection{Find and repair gaps in
dataset}\label{find-and-repair-gaps-in-dataset}}

This section was inspired by
\href{https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/}{Working
with Missing Data in Pandas}.

\hypertarget{check-for-missing-values-using-isnull}{%
\paragraph{\texorpdfstring{Check for missing values using
\texttt{isnull()}}{Check for missing values using isnull()}}\label{check-for-missing-values-using-isnull}}

In order to check for missing values in Pandas DataFrame, we use the
function \texttt{isnull()}. This function returns a dataframe of Boolean
values which are True for \textbf{NaN values}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{101}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.max\PYZus{}rows}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{40}\PY{p}{)}
\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.min\PYZus{}rows}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+m+mi}{30}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{102}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{102}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     sepal\_length  sepal\_width  petal\_length  petal\_width  species
0           False        False         False        False    False
1           False        False         False        False    False
2           False        False         False        False    False
3           False        False         False        False    False
4           False        False         False        False    False
5           False        False         False        False    False
6           False        False         False        False    False
7           False        False         False        False    False
8           False        False         False        False    False
9           False        False         False        False    False
10          False        False         False        False    False
11          False        False         False        False    False
12          False        False         False        False    False
13          False        False         False        False    False
14          False        False         False        False    False
..            {\ldots}          {\ldots}           {\ldots}          {\ldots}      {\ldots}
135         False        False         False        False    False
136         False        False         False        False    False
137         False        False         False        False    False
138         False        False         False        False    False
139         False        False         False        False    False
140         False        False         False        False    False
141         False        False         False        False    False
142         False        False         False        False    False
143         False        False         False        False    False
144         False        False         False        False    False
145         False        False         False        False    False
146         False        False         False        False    False
147         False        False         False        False    False
148         False        False         False        False    False
149         False        False         False        False    False

[150 rows x 5 columns]
\end{Verbatim}
\end{tcolorbox}
        
    Show only the gaps:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{103}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df\PYZus{}gaps} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
\PY{n}{irisdata\PYZus{}df\PYZus{}gaps}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{103}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Empty DataFrame
Columns: [sepal\_length, sepal\_width, petal\_length, petal\_width, species]
Index: []
\end{Verbatim}
\end{tcolorbox}
        
    Fine - this dataset seems to be complete :)

So let's look for something else for exercise:
\href{https://media.geeksforgeeks.org/wp-content/uploads/employees.csv}{employes.csv}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{145}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import data to dataframe from csv file}
\PY{n}{employees\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./datasets/employees\PYZus{}edit.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} highlight cells with NaN values}
\PY{c+c1}{\PYZsh{} HINT: Set to \PYZsq{}False\PYZsq{} when compiling to PDF!}
\PY{n}{highlight} \PY{o}{=} \PY{k+kc}{False}

\PY{n}{employees\PYZus{}df\PYZus{}hl} \PY{o}{=} \PY{n}{employees\PYZus{}df}

\PY{k}{if} \PY{n}{highlight}\PY{p}{:}
    \PY{n}{employees\PYZus{}df\PYZus{}hl} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{highlight\PYZus{}null}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{employees\PYZus{}df\PYZus{}hl}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{145}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     First Name  Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
0       Douglas    Male    8/6/1993        12:42 PM   97308   6945.00
1        Thomas    Male   3/31/1996         6:53 AM   61933      4.17
2         Maria  Female   4/23/1993        11:17 AM  130590  11858.00
3         Jerry    Male    3/4/2005         1:00 PM  138705      9.34
4         Larry    Male   1/24/1998         4:47 PM  101004   1389.00
5        Dennis    Male   4/18/1987         1:35 AM  115163  10125.00
6          Ruby  Female   8/17/1987         4:20 PM   65476  10012.00
7           NaN  Female   7/20/2015        10:43 AM   45906  11598.00
8        Angela  Female  11/22/2005         6:29 AM   95570  18523.00
9       Frances  Female    8/8/2002         6:51 AM  139852   7524.00
10       Louise  Female   8/12/1980         9:01 AM   63241  15132.00
11        Julie  Female  10/26/1997         3:19 PM  102508  12637.00
12      Brandon    Male   12/1/1980         1:08 AM  112807  17492.00
13         Gary    Male   1/27/2008        11:40 PM  109831   5831.00
14     Kimberly  Female   1/14/1999         7:13 AM   41426  14543.00
{\ldots}         {\ldots}     {\ldots}         {\ldots}             {\ldots}     {\ldots}       {\ldots}
989     Stephen     NaN   7/10/1983         8:10 PM   85668   1909.00
990       Donna  Female  11/26/1982         7:04 AM   82871  17999.00
991      Gloria  Female   12/8/2014         5:08 AM  136709  10331.00
992       Alice  Female   10/5/2004         9:34 AM   47638  11209.00
993      Justin     NaN   2/10/1991         4:58 PM   38344   3794.00
994       Robin  Female   7/24/1987         1:35 PM  100765  10982.00
995        Rose  Female   8/25/2002         5:12 AM  134505  11051.00
996     Anthony    Male  10/16/2011         8:35 AM  112769  11625.00
997        Tina  Female   5/15/1997         3:53 PM   56450     19.04
998      George    Male   6/21/2013         5:47 PM   98874   4479.00
999       Henry     NaN  11/23/2014         6:09 AM  132483  16655.00
1000    Phillip    Male   1/31/1984         6:30 AM   42392  19675.00
1001    Russell    Male   5/20/2013        12:39 PM   96914   1421.00
1002      Larry    Male   4/20/2013         4:45 PM   60500  11985.00
1003     Albert    Male   5/15/2012         6:24 PM  129949  10169.00

     Senior Management                  Team
0                 True             Marketing
1                 True                   NaN
2                False               Finance
3                 True               Finance
4                 True       Client Services
5                False                 Legal
6                 True               Product
7                  NaN               Finance
8                 True           Engineering
9                 True  Business Development
10                True                   NaN
11                True                 Legal
12                True       Human Resources
13               False                 Sales
14                True               Finance
{\ldots}                {\ldots}                   {\ldots}
989              False                 Legal
990              False             Marketing
991               True               Finance
992              False       Human Resources
993              False                 Legal
994               True       Client Services
995               True             Marketing
996               True               Finance
997               True           Engineering
998               True             Marketing
999              False          Distribution
1000             False               Finance
1001             False               Product
1002             False  Business Development
1003              True                 Sales

[1004 rows x 8 columns]
\end{Verbatim}
\end{tcolorbox}
        
    Show only the gaps from this gappy dataset again:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{146}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{employees\PYZus{}df\PYZus{}gaps} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}

\PY{c+c1}{\PYZsh{} highlight cells with NaN values}
\PY{c+c1}{\PYZsh{} HINT: Set to \PYZsq{}False\PYZsq{} when compiling to PDF!}
\PY{n}{highlight} \PY{o}{=} \PY{k+kc}{False}

\PY{k}{if} \PY{n}{highlight}\PY{p}{:}
    \PY{n}{employees\PYZus{}df\PYZus{}gaps} \PY{o}{=} \PY{n}{employees\PYZus{}df\PYZus{}gaps}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{highlight\PYZus{}null}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{employees\PYZus{}df\PYZus{}gaps}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{146}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    First Name  Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
1       Thomas    Male   3/31/1996         6:53 AM   61933      4.17
7          NaN  Female   7/20/2015        10:43 AM   45906  11598.00
10      Louise  Female   8/12/1980         9:01 AM   63241  15132.00
20        Lois     NaN   4/22/1995         7:18 PM   64714   4934.00
22      Joshua     NaN    3/8/2012         1:58 AM   90816  18816.00
23         NaN    Male   6/14/2012         4:19 PM  125792   5042.00
25         NaN    Male   10/8/2012         1:12 AM   37076  18576.00
27       Scott     NaN   7/11/1991         6:58 PM  122367   5218.00
31       Joyce     NaN   2/20/2005         2:40 PM   88657  12752.00
32         NaN    Male   8/21/1998         2:27 PM  122340   6417.00
39         NaN    Male   1/29/2016         2:33 AM  122173   7797.00
41   Christine     NaN   6/28/2015         1:08 AM   66582  11308.00
49       Chris     NaN   1/24/1980        12:13 PM  113590   3055.00
51         NaN     NaN  12/17/2011         8:29 AM   41126  14009.00
53        Alan     NaN    3/3/2014         1:28 PM   40341  17578.00
..         {\ldots}     {\ldots}         {\ldots}             {\ldots}     {\ldots}       {\ldots}
916        Joe    Male   12/8/1998        10:28 AM  126120      1.02
927      Irene     NaN   2/28/1991        10:23 PM  135369      4.38
929        NaN  Female   8/23/2000         4:19 PM   95866  19388.00
941      Aaron     NaN   1/22/1986         7:39 PM   63126  18424.00
942       Mark     NaN    9/9/2006        12:27 PM   44836   2657.00
943      Ralph     NaN   7/28/1995         6:53 PM   70635   2147.00
949     Gerald     NaN   4/15/1989        12:44 PM   93712  17426.00
950        NaN  Female   9/15/1985         1:50 AM  133472  16941.00
951        NaN    Male   7/30/2012         3:07 PM  107351   5329.00
955        NaN  Female   9/14/2010         5:19 AM  143638   9662.00
965    Antonio     NaN   6/18/1989         9:37 PM  103050      3.05
976     Victor     NaN   7/28/2006         2:49 PM   76381  11159.00
989    Stephen     NaN   7/10/1983         8:10 PM   85668   1909.00
993     Justin     NaN   2/10/1991         4:58 PM   38344   3794.00
999      Henry     NaN  11/23/2014         6:09 AM  132483  16655.00

    Senior Management                  Team
1                True                   NaN
7                 NaN               Finance
10               True                   NaN
20               True                 Legal
22               True       Client Services
23                NaN                   NaN
25                NaN       Client Services
27              False                 Legal
31              False               Product
32                NaN                   NaN
39                NaN       Client Services
41               True  Business Development
49              False                 Sales
51                NaN                 Sales
53               True               Finance
..                {\ldots}                   {\ldots}
916             False                   NaN
927             False  Business Development
929               NaN                 Sales
941             False       Client Services
942             False       Client Services
943             False       Client Services
949              True          Distribution
950               NaN          Distribution
951               NaN             Marketing
955               NaN                   NaN
965             False                 Legal
976              True                 Sales
989             False                 Legal
993             False                 Legal
999             False          Distribution

[237 rows x 8 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{fill-in-missing-string-values-with-fillna}{%
\paragraph{\texorpdfstring{Fill in missing \emph{string} values with
\texttt{fillna()}}{Fill in missing string values with fillna()}}\label{fill-in-missing-string-values-with-fillna}}

Now all null values (NaN) in the column ``Gender'' of the data type
String are filled with \emph{``No gender''}.

\textbf{Warning:} We are doing that directly in this dataframe with
\texttt{inplace\ =\ True} - we don't make a deep copy!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{147}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} filling a null values using fillna()}
\PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gender}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No Gender}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{inplace} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} switch to apply highlight style to dataframe}
\PY{c+c1}{\PYZsh{} HINT: Set to \PYZsq{}False\PYZsq{} when compiling to PDF!}
\PY{n}{highlight} \PY{o}{=} \PY{k+kc}{False}

\PY{n}{employees\PYZus{}df\PYZus{}filled} \PY{o}{=} \PY{n}{employees\PYZus{}df}

\PY{k}{if} \PY{n}{highlight}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} highlight cells by condition}
    \PY{n}{employees\PYZus{}df\PYZus{}filled} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} 
                                                   \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{background: yellow}\PY{l+s+s2}{\PYZdq{}} 
                                                    \PY{k}{if} \PY{n}{v} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No Gender}\PY{l+s+s1}{\PYZsq{}} 
                                                    \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}} \PY{k}{for} \PY{n}{v} \PY{o+ow}{in} \PY{n}{x}\PY{p}{]}\PY{p}{,} 
                                                   \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{employees\PYZus{}df\PYZus{}filled}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{147}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     First Name     Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
0       Douglas       Male    8/6/1993        12:42 PM   97308   6945.00
1        Thomas       Male   3/31/1996         6:53 AM   61933      4.17
2         Maria     Female   4/23/1993        11:17 AM  130590  11858.00
3         Jerry       Male    3/4/2005         1:00 PM  138705      9.34
4         Larry       Male   1/24/1998         4:47 PM  101004   1389.00
5        Dennis       Male   4/18/1987         1:35 AM  115163  10125.00
6          Ruby     Female   8/17/1987         4:20 PM   65476  10012.00
7           NaN     Female   7/20/2015        10:43 AM   45906  11598.00
8        Angela     Female  11/22/2005         6:29 AM   95570  18523.00
9       Frances     Female    8/8/2002         6:51 AM  139852   7524.00
10       Louise     Female   8/12/1980         9:01 AM   63241  15132.00
11        Julie     Female  10/26/1997         3:19 PM  102508  12637.00
12      Brandon       Male   12/1/1980         1:08 AM  112807  17492.00
13         Gary       Male   1/27/2008        11:40 PM  109831   5831.00
14     Kimberly     Female   1/14/1999         7:13 AM   41426  14543.00
{\ldots}         {\ldots}        {\ldots}         {\ldots}             {\ldots}     {\ldots}       {\ldots}
989     Stephen  No Gender   7/10/1983         8:10 PM   85668   1909.00
990       Donna     Female  11/26/1982         7:04 AM   82871  17999.00
991      Gloria     Female   12/8/2014         5:08 AM  136709  10331.00
992       Alice     Female   10/5/2004         9:34 AM   47638  11209.00
993      Justin  No Gender   2/10/1991         4:58 PM   38344   3794.00
994       Robin     Female   7/24/1987         1:35 PM  100765  10982.00
995        Rose     Female   8/25/2002         5:12 AM  134505  11051.00
996     Anthony       Male  10/16/2011         8:35 AM  112769  11625.00
997        Tina     Female   5/15/1997         3:53 PM   56450     19.04
998      George       Male   6/21/2013         5:47 PM   98874   4479.00
999       Henry  No Gender  11/23/2014         6:09 AM  132483  16655.00
1000    Phillip       Male   1/31/1984         6:30 AM   42392  19675.00
1001    Russell       Male   5/20/2013        12:39 PM   96914   1421.00
1002      Larry       Male   4/20/2013         4:45 PM   60500  11985.00
1003     Albert       Male   5/15/2012         6:24 PM  129949  10169.00

     Senior Management                  Team
0                 True             Marketing
1                 True                   NaN
2                False               Finance
3                 True               Finance
4                 True       Client Services
5                False                 Legal
6                 True               Product
7                  NaN               Finance
8                 True           Engineering
9                 True  Business Development
10                True                   NaN
11                True                 Legal
12                True       Human Resources
13               False                 Sales
14                True               Finance
{\ldots}                {\ldots}                   {\ldots}
989              False                 Legal
990              False             Marketing
991               True               Finance
992              False       Human Resources
993              False                 Legal
994               True       Client Services
995               True             Marketing
996               True               Finance
997               True           Engineering
998               True             Marketing
999              False          Distribution
1000             False               Finance
1001             False               Product
1002             False  Business Development
1003              True                 Sales

[1004 rows x 8 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{fill-in-missing-numerical-values-with-median-values}{%
\paragraph{\texorpdfstring{Fill in missing \emph{numerical} values with
median
values}{Fill in missing numerical values with median values}}\label{fill-in-missing-numerical-values-with-median-values}}

Missing integer or float values can be filled with the \textbf{median
values of the corresponding column}.

\textbf{@TODO:}\\
Incorporate section ``4.1.3 Fehlende Werte ergänzen'' of the book
\texttt{mitp\_Praxishandbuch\_Machine\_Learning\_Python\_Scikit-learn\_TensorFlow\_2018\_Anm\_bk.pdf}
(see \cite{ML_ScL_2018}).

\begin{itemize}
\tightlist
\item
  https://www.statology.org/pandas-fillna-with-median/
\item
  https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns
\end{itemize}

    \hypertarget{drop-missing-values-using-dropna}{%
\paragraph{\texorpdfstring{Drop missing values using
\texttt{dropna()}}{Drop missing values using dropna()}}\label{drop-missing-values-using-dropna}}

In order to drop null values from a dataframe, we use \texttt{dropna()}
function. This function drops rows or columns of datasets with NaN
values in different ways.

Default is to drop rows with at least 1 null value (NaN). Giving the
parameter \texttt{how\ =\ \textquotesingle{}all\textquotesingle{}} the
function drops rows with all data missing or contain null values (NaN).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} making a new dataframe with dropped NaN values}
\PY{n}{employees\PYZus{}df\PYZus{}dropped} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{how} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{any}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{employees\PYZus{}df\PYZus{}dropped}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     First Name     Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
0       Douglas       Male    8/6/1993        12:42 PM   97308   6945.00
2         Maria     Female   4/23/1993        11:17 AM  130590  11858.00
3         Jerry       Male    3/4/2005         1:00 PM  138705      9.34
4         Larry       Male   1/24/1998         4:47 PM  101004   1389.00
5        Dennis       Male   4/18/1987         1:35 AM  115163  10125.00
{\ldots}         {\ldots}        {\ldots}         {\ldots}             {\ldots}     {\ldots}       {\ldots}
999       Henry  No Gender  11/23/2014         6:09 AM  132483  16655.00
1000    Phillip       Male   1/31/1984         6:30 AM   42392  19675.00
1001    Russell       Male   5/20/2013        12:39 PM   96914   1421.00
1002      Larry       Male   4/20/2013         4:45 PM   60500  11985.00
1003     Albert       Male   5/15/2012         6:24 PM  129949  10169.00

     Senior Management                  Team
0                 True             Marketing
2                False               Finance
3                 True               Finance
4                 True       Client Services
5                False                 Legal
{\ldots}                {\ldots}                   {\ldots}
999              False          Distribution
1000             False               Finance
1001             False               Product
1002             False  Business Development
1003              True                 Sales

[903 rows x 8 columns]
\end{Verbatim}
\end{tcolorbox}
        
    Finally we compare the sizes of dataframes so that we learn how many
rows had at least 1 Null value.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Old data frame length:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{employees\PYZus{}df}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{New data frame length:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{employees\PYZus{}df\PYZus{}dropped}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of rows with at least 1 NaN value: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
      \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{employees\PYZus{}df}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{employees\PYZus{}df\PYZus{}dropped}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Old data frame length: 1004
New data frame length: 903
Number of rows with at least 1 NaN value:  101
    \end{Verbatim}

    \hypertarget{find-and-remove-duplicates-in-dataset}{%
\subsubsection{Find and remove duplicates in
dataset}\label{find-and-remove-duplicates-in-dataset}}

This section was inspired by: -
\href{https://www.statology.org/pandas-find-duplicates/}{How to Find
Duplicates in Pandas DataFrame (With Examples)} -
\href{https://www.statology.org/pandas-drop-duplicates/}{How to Drop
Duplicate Rows in a Pandas DataFrame}

    \hypertarget{check-for-duplicate-values-using-duplicated}{%
\paragraph{\texorpdfstring{Check for duplicate values using
\texttt{duplicated()}}{Check for duplicate values using duplicated()}}\label{check-for-duplicate-values-using-duplicated}}

In order to check for duplicate values in Pandas DataFrame, we use a
function \texttt{duplicated()}. This function can be used in two ways: -
find duplicate rows across \textbf{all columns} with
\texttt{duplicateRows\ =\ df{[}df.duplicated(){]}} - find duplicate rows
across \textbf{specific columns}
\texttt{duplicateRows\ =\ df{[}df.duplicated(subset={[}\textquotesingle{}col1\textquotesingle{},\ \textquotesingle{}col2\textquotesingle{}{]}){]}}

Find duplicate rows across \textbf{all columns}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import (again) data to dataframe from csv file}
\PY{n}{employees\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./datasets/employees\PYZus{}edit.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} find duplicate rows across all columns}
\PY{n}{duplicateRows} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{p}{)}\PY{p}{]}
\PY{n}{duplicateRows}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    First Name  Gender  Start Date Last Login Time  Salary  Bonus \%  \textbackslash{}
112      Karen  Female  11/30/1999         7:46 AM  102488  17653.0
127      Linda  Female   5/25/2000         5:45 PM  119009  12506.0
296    Brandon     NaN   11/3/1997         8:17 PM  121333  15295.0
580   Nicholas    Male    3/1/2013         9:26 PM  101036   2826.0

    Senior Management                  Team
112              True               Product
127              True  Business Development
296             False  Business Development
580              True       Human Resources
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} argument keep=’last’ displays the first duplicate rows instead of the last}
\PY{n}{duplicateRows} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}\PY{n}{keep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{last}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
\PY{n}{duplicateRows}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    First Name  Gender  Start Date Last Login Time  Salary  Bonus \%  \textbackslash{}
55       Karen  Female  11/30/1999         7:46 AM  102488  17653.0
92       Linda  Female   5/25/2000         5:45 PM  119009  12506.0
153    Brandon     NaN   11/3/1997         8:17 PM  121333  15295.0
442   Nicholas    Male    3/1/2013         9:26 PM  101036   2826.0

    Senior Management                  Team
55               True               Product
92               True  Business Development
153             False  Business Development
442              True       Human Resources
\end{Verbatim}
\end{tcolorbox}
        
    Find duplicate rows across \textbf{specific columns}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} identify duplicate rows across \PYZsq{}First Name\PYZsq{} and \PYZsq{}Last Login Time\PYZsq{} columns}
\PY{n}{duplicateRows} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}
                    \PY{n}{subset}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{First Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Last Login Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}
\PY{n}{duplicateRows}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    First Name  Gender  Start Date Last Login Time  Salary  Bonus \%  \textbackslash{}
112      Karen  Female  11/30/1999         7:46 AM  102488  17653.0
127      Linda  Female   5/25/2000         5:45 PM  119009  12506.0
296    Brandon     NaN   11/3/1997         8:17 PM  121333  15295.0
577        NaN  Female   1/13/2009         1:01 PM  118736   7421.0
580   Nicholas    Male    3/1/2013         9:26 PM  101036   2826.0
632        NaN     NaN    9/2/1988        12:49 PM  147309   1702.0
881        NaN    Male    9/5/1980         7:36 AM  114896  13823.0
929        NaN  Female   8/23/2000         4:19 PM   95866  19388.0
934      Nancy  Female   9/10/2001        11:57 PM   85213   2386.0
973      Linda  Female    2/4/2010         8:49 PM   44486  17308.0

    Senior Management                  Team
112              True               Product
127              True  Business Development
296             False  Business Development
577               NaN       Client Services
580              True       Human Resources
632               NaN          Distribution
881               NaN       Client Services
929               NaN                 Sales
934              True             Marketing
973              True           Engineering
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} argument keep=’last’ displays the first duplicate rows instead of the last}
\PY{n}{duplicateRows} \PY{o}{=} \PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{duplicated}\PY{p}{(}
                    \PY{n}{subset}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{First Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Last Login Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{keep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{last}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
\PY{n}{duplicateRows}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    First Name  Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
23         NaN    Male   6/14/2012         4:19 PM  125792   5042.00
37       Linda  Female  10/19/1981         8:49 PM   57427   9557.00
55       Karen  Female  11/30/1999         7:46 AM  102488  17653.00
66       Nancy  Female  12/15/2012        11:57 PM  125250   2672.00
92       Linda  Female   5/25/2000         5:45 PM  119009  12506.00
153    Brandon     NaN   11/3/1997         8:17 PM  121333  15295.00
222        NaN  Female   6/17/1991        12:49 PM   71945      5.56
269        NaN    Male    2/4/2005         1:01 PM   40451  16044.00
442   Nicholas    Male    3/1/2013         9:26 PM  101036   2826.00
778        NaN  Female   6/18/2000         7:36 AM  106428  10867.00

    Senior Management                  Team
23                NaN                   NaN
37               True       Client Services
55               True               Product
66               True  Business Development
92               True  Business Development
153             False  Business Development
222               NaN             Marketing
269               NaN          Distribution
442              True       Human Resources
778               NaN                   NaN
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{drop-duplicate-values-using-drop_duplicates}{%
\paragraph{\texorpdfstring{Drop duplicate values using
\texttt{drop\_duplicates()}}{Drop duplicate values using drop\_duplicates()}}\label{drop-duplicate-values-using-drop_duplicates}}

In order to drop duplicate values from a dataframe, we use
\texttt{drop\_duplicates()} function.

This function can be used in two ways: - remove duplicate rows across
\textbf{all columns} with \texttt{df.drop\_duplicates()} - find
duplicate rows across \textbf{specific columns}
\texttt{df.drop\_duplicates(subset={[}\textquotesingle{}col1\textquotesingle{},\ \textquotesingle{}col2\textquotesingle{}{]})}

\textbf{Warning:} We are doing that directly in this dataframe with
\texttt{inplace\ =\ True} - we don't make a deep copy!

Remove duplicate rows across \textbf{all columns}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} remove duplicate rows across all columns}
\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{employees\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     First Name  Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
0       Douglas    Male    8/6/1993        12:42 PM   97308   6945.00
1        Thomas    Male   3/31/1996         6:53 AM   61933      4.17
2         Maria  Female   4/23/1993        11:17 AM  130590  11858.00
3         Jerry    Male    3/4/2005         1:00 PM  138705      9.34
4         Larry    Male   1/24/1998         4:47 PM  101004   1389.00
{\ldots}         {\ldots}     {\ldots}         {\ldots}             {\ldots}     {\ldots}       {\ldots}
999       Henry     NaN  11/23/2014         6:09 AM  132483  16655.00
1000    Phillip    Male   1/31/1984         6:30 AM   42392  19675.00
1001    Russell    Male   5/20/2013        12:39 PM   96914   1421.00
1002      Larry    Male   4/20/2013         4:45 PM   60500  11985.00
1003     Albert    Male   5/15/2012         6:24 PM  129949  10169.00

     Senior Management                  Team
0                 True             Marketing
1                 True                   NaN
2                False               Finance
3                 True               Finance
4                 True       Client Services
{\ldots}                {\ldots}                   {\ldots}
999              False          Distribution
1000             False               Finance
1001             False               Product
1002             False  Business Development
1003              True                 Sales

[1000 rows x 8 columns]
\end{Verbatim}
\end{tcolorbox}
        
    Remove duplicate rows across \textbf{specific columns}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} remove duplicate rows across \PYZsq{}First Name\PYZsq{} and \PYZsq{}Last Login Time\PYZsq{} columns}
\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}
    \PY{n}{subset}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{First Name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Last Login Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{keep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{last}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{employees\PYZus{}df}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     First Name  Gender  Start Date Last Login Time  Salary   Bonus \%  \textbackslash{}
0       Douglas    Male    8/6/1993        12:42 PM   97308   6945.00
1        Thomas    Male   3/31/1996         6:53 AM   61933      4.17
2         Maria  Female   4/23/1993        11:17 AM  130590  11858.00
3         Jerry    Male    3/4/2005         1:00 PM  138705      9.34
4         Larry    Male   1/24/1998         4:47 PM  101004   1389.00
{\ldots}         {\ldots}     {\ldots}         {\ldots}             {\ldots}     {\ldots}       {\ldots}
999       Henry     NaN  11/23/2014         6:09 AM  132483  16655.00
1000    Phillip    Male   1/31/1984         6:30 AM   42392  19675.00
1001    Russell    Male   5/20/2013        12:39 PM   96914   1421.00
1002      Larry    Male   4/20/2013         4:45 PM   60500  11985.00
1003     Albert    Male   5/15/2012         6:24 PM  129949  10169.00

     Senior Management                  Team
0                 True             Marketing
1                 True                   NaN
2                False               Finance
3                 True               Finance
4                 True       Client Services
{\ldots}                {\ldots}                   {\ldots}
999              False          Distribution
1000             False               Finance
1001             False               Product
1002             False  Business Development
1003              True                 Sales

[994 rows x 8 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{compare-the-edited-dataset-with-the-original-dataset-side-by-side}{%
\subsubsection{Compare the edited dataset with the original dataset
side-by-side}\label{compare-the-edited-dataset-with-the-original-dataset-side-by-side}}

\textbf{@TODO:}\\
Incorporate following sources: -
\href{https://stackoverflow.com/questions/17095101/compare-two-dataframes-and-output-their-differences-side-by-side/47112033\#47112033}{Compare
two DataFrames and output their differences side-by-side} -
\href{https://stackoverflow.com/questions/71604701/pandas-compare-two-data-frames-and-highlight-the-differences/71617662\#71617662}{pandas
compare two data frames and highlight the differences} -
\href{https://datascientyst.com/compare-two-pandas-dataframes-get-differences/}{How
to Compare Two Pandas DataFrames and Get Differences}

    \hypertarget{save-edited-dataset-to-new-csv-file}{%
\subsubsection{Save edited dataset to new CSV
file}\label{save-edited-dataset-to-new-csv-file}}

\textbf{@TODO:}\\
Add explanation and python code here.

    \hypertarget{avoidance-of-tendencies-due-to-bias}{%
\subsection{\texorpdfstring{Avoidance of \textbf{tendencies due to
bias}}{Avoidance of tendencies due to bias}}\label{avoidance-of-tendencies-due-to-bias}}

The description of the Iris dataset says, that it consists of \textbf{50
samples} from \textbf{each of three species} of Iris (Iris setosa, Iris
virginica and Iris versicolor), so there are \textbf{150 total samples}.

But how to prove it?

\hypertarget{count-occurrences-of-unique-values}{%
\subsubsection{Count occurrences of unique
values}\label{count-occurrences-of-unique-values}}

To prove whether all possible classes included in the dataset and
equally distributed, you can use the function \texttt{df.value\_counts}.

Following parameters can be used for fine tuning: -
\texttt{dropna=False} causes that NaN values are included -
\texttt{normalize=True}: relative frequencies of the unique values are
returned - \texttt{ascending=False}: sort resulting classes descending

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import (again) data to dataframe from csv file}
\PY{n}{employees\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./datasets/employees\PYZus{}edit.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} count unique values without missing values in a column, }
\PY{c+c1}{\PYZsh{} ordered descending and normalized}
\PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{dropna}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Iris-setosa        0.333333
Iris-versicolor    0.333333
Iris-virginica     0.333333
Name: species, dtype: float64
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} count unique values and missing values in a column, }
\PY{c+c1}{\PYZsh{} ordered descending and not absolute values}
\PY{n}{employees\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Team}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{dropna}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Client Services         106
Business Development    103
Finance                 102
Marketing                98
Product                  96
Sales                    94
Engineering              92
Human Resources          92
Distribution             90
Legal                    88
NaN                      43
Name: Team, dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{display-histogram}{%
\subsubsection{Display Histogram}\label{display-histogram}}

This section was inspired by:
\href{https://dataindependent.com/pandas/pandas-histogram-dataframe-hist/}{Pandas
Histogram -- DataFrame.hist()}.

\textbf{Histograms} represent \textbf{frequency distributions}
graphically. This requires the separation of the data into classes
(so-called \textbf{bins}).

These classes are represented in the histogram as rectangles of equal or
variable width. The height of each rectangle then represents the
(relative or absolute) \textbf{frequency density}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{column}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Salary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_92_0.png}\end{center}
        \caption{Histogram for frequency distribution of the salary}
        \label{fig:histogram_salary}
    \end{figure}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{employees\PYZus{}df}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{column}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Salary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gender}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_93_0.png}\end{center}
        \caption{Histogram for the frequency distribution of the salary in comparison between men and women}
        \label{fig:histogram_male_female}
    \end{figure}
    
    \hypertarget{first-idea-of-correlations-in-dataset}{%
\subsection{\texorpdfstring{First \textbf{idea of correlations} in
dataset}{First idea of correlations in dataset}}\label{first-idea-of-correlations-in-dataset}}

To get a rough idea of the \textbf{dependencies} and
\textbf{correlations} in the dataset, it can be helpful to visualize the
whole dataset in a \textbf{correlation heatmap}. They show in a glance
which variables are correlated, to what degree and in which direction.

Later, 2 particularly well correlated variables are selected from the
dataset and plotted in a \textbf{scatterplot}.

    \hypertarget{visualise-data-with-correlation-heatmap}{%
\subsubsection{\texorpdfstring{Visualise data with \textbf{correlation
heatmap}}{Visualise data with correlation heatmap}}\label{visualise-data-with-correlation-heatmap}}

This section was inspired by
\href{https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e}{How
to Create a Seaborn Correlation Heatmap in Python?}.

\begin{quote}
\textbf{Correlation matrices} are an \textbf{essential tool of
exploratory data analysis}. Correlation heatmaps contain the same
information in a visually appealing way. What more: they show in a
glance which variables are correlated, to what degree, in which
direction, and alerts us to potential multicollinearity problems
(source: ibidem).
\end{quote}

\hypertarget{simple-correlation-matrix}{%
\paragraph{Simple correlation matrix}\label{simple-correlation-matrix}}

Because \textbf{string values can never be correlated}, the class names
(species) have to be converted first:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} encoding the class column}
\PY{n}{irisdata\PYZus{}df\PYZus{}enc} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{species}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}  \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iris\PYZhy{}setosa}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{0}\PY{p}{,}
                                                    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iris\PYZhy{}versicolor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{,} 
                                                    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iris\PYZhy{}virginica}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{\PYZcb{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}irisdata\PYZus{}df\PYZus{}enc}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              sepal\_length  sepal\_width  petal\_length  petal\_width   species
sepal\_length      1.000000    -0.109369      0.871754     0.817954  0.782561
sepal\_width      -0.109369     1.000000     -0.420516    -0.356544 -0.419446
petal\_length      0.871754    -0.420516      1.000000     0.962757  0.949043
petal\_width       0.817954    -0.356544      0.962757     1.000000  0.956464
species           0.782561    -0.419446      0.949043     0.956464  1.000000
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{correlation-heatmap}{%
\paragraph{Correlation heatmap}\label{correlation-heatmap}}

Choose the color sets from
\href{https://pod.hatenablog.com/entry/2018/09/20/212527}{color map}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} increase the size of the heatmap}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} store heatmap object in a variable to easily access it }
\PY{c+c1}{\PYZsh{} when you want to include more features (such as title)}
\PY{c+c1}{\PYZsh{} set the range of values to be displayed on the colormap from \PYZhy{}1 to 1,}
\PY{c+c1}{\PYZsh{} and set \PYZsq{}annotation=True\PYZsq{} to display the correlation values on the heatmap}
\PY{n}{heatmap} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{vmin}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} 
                      \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PRGn\PYZus{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} give a title to the heatmap}
\PY{c+c1}{\PYZsh{} \PYZsq{}pad=12\PYZsq{} defines the distance of the title from the top of the heatmap}
\PY{n}{heatmap}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Correlation Heatmap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontdict}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fontsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{18}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{pad}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_99_0.png}\end{center}
        \caption{Correlation heatmap to explore coherences between single variables in the iris dataset}
        \label{fig:correlation_heatmap}
    \end{figure}
    
    \hypertarget{triangle-correlation-heatmap}{%
\paragraph{Triangle correlation
heatmap}\label{triangle-correlation-heatmap}}

When looking at the correlation heatmaps above, you would not lose any
information by \textbf{cutting} away half of it \textbf{along the
diagonal} line marked by 1-s.

The \textbf{numpy} function \texttt{np.triu()} can be used to isolate
the upper triangle of a matrix while turning all the values in the lower
triangle into 0.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{np}\PY{o}{.}\PY{n}{triu}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[1., 1., 1., 1., 1.],
       [0., 1., 1., 1., 1.],
       [0., 0., 1., 1., 1.],
       [0., 0., 0., 1., 1.],
       [0., 0., 0., 0., 1.]])
\end{Verbatim}
\end{tcolorbox}
        
    Use this mask to cut the heatmap along the diagonal:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} define the mask to set the values in the upper triangle to \PYZsq{}True\PYZsq{}}
\PY{n}{mask} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{triu}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones\PYZus{}like}\PY{p}{(}\PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{bool}\PY{p}{)}\PY{p}{)}

\PY{n}{heatmap} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{mask}\PY{o}{=}\PY{n}{mask}\PY{p}{,} 
                      \PY{n}{vmin}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PRGn\PYZus{}r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{heatmap}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Triangle Correlation Heatmap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontdict}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fontsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+m+mi}{18}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{pad}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_103_0.png}\end{center}
        \caption{Correlation heatmap, which was cut at its main diagonal without losing any information}
        \label{fig:correlation_heatmap_triangle}
    \end{figure}
    
    As a result from the \textbf{heatmaps} we can see, that the shape of the
\textbf{petals} are the \textbf{most correlationed columns} (0.96) with
the \textbf{type of flowers} (species classes).

Somewhat lower correlates \textbf{sepal length} with \textbf{petal
length} (0.87).

    \hypertarget{visualise-data-with-scatter-plot}{%
\subsubsection{\texorpdfstring{Visualise data with \textbf{scatter
plot}}{Visualise data with scatter plot}}\label{visualise-data-with-scatter-plot}}

In the following, \href{https://seaborn.pydata.org/}{Seaborn} is applied
which is a library for making statistical graphics in Python. It is
built on top of matplotlib and closely integrated with pandas data
structures.

To investigate whether there are dependencies (e.g.~correlations) in
\texttt{irisdata\_df} between individual variables in the dataset, it is
advisable to plot them in a \textbf{scatter plot}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} There are five preset seaborn themes: darkgrid, whitegrid, dark, white, and ticks.}
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} set scale of fonts}
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}context}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{notebook}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.3}\PY{p}{,} \PY{n}{rc}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lines.linewidth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mf}{2.5}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} \PYZsq{}sepal\PYZus{}length\PYZsq{}, \PYZsq{}petal\PYZus{}length\PYZsq{} are iris feature data}
\PY{c+c1}{\PYZsh{} \PYZsq{}height\PYZsq{} used to define height of graph}
\PY{c+c1}{\PYZsh{} \PYZsq{}hue\PYZsq{} stores the class/label of iris dataset}
\PY{n}{sns}\PY{o}{.}\PY{n}{FacetGrid}\PY{p}{(}\PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{hue} \PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{species}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
              \PY{n}{height} \PY{o}{=} \PY{l+m+mi}{7}\PY{p}{)}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{,}
                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{add\PYZus{}legend}\PY{p}{(}\PY{p}{)}


\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scatterplot of petal length and width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_106_0.png}\end{center}
        \caption{Plotting two individual variables of the iris dataset in the scatterplot to explore the relationships between these two}
        \label{fig:scatter_plot}
    \end{figure}
    
    \hypertarget{visualise-data-with-pairs-plot}{%
\subsubsection{\texorpdfstring{Visualise data with \textbf{pairs
plot}}{Visualise data with pairs plot}}\label{visualise-data-with-pairs-plot}}

For systematic investigation of dependencies, all variables (each
against each) are plotted in separate scatter plots.

With this so called
\textbf{\href{https://vita.had.co.nz/papers/gpp.pdf}{pairs plot}} it is
possible to see both \textbf{relationships} between two variables and
\textbf{distribution} of single variables.

This function will create a grid of Axes such that \textbf{each numeric
variable} in \texttt{irisdata\_df} will by shared in the y-axis across a
single row and in the x-axis across a single column.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{g} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{irisdata\PYZus{}df}\PY{p}{,} \PY{n}{diag\PYZus{}kind}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kde}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                 \PY{n}{palette}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dark2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{height}\PY{o}{=}\PY{l+m+mf}{2.5}\PY{p}{)}

\PY{n}{g}\PY{o}{.}\PY{n}{map\PYZus{}lower}\PY{p}{(}\PY{n}{sns}\PY{o}{.}\PY{n}{kdeplot}\PY{p}{,} \PY{n}{levels}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} y .. padding between title and plot}
\PY{n}{g}\PY{o}{.}\PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Pairs plot of the Iris dataset}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+m+mf}{1.05}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_108_0.png}\end{center}
        \caption{Plot all individual variables of the Iris dataset in pairs plot to see both the relationships between two variables and the distribution of the individual variables}
        \label{fig:pairs_plot}
    \end{figure}
    
    \hypertarget{step-3-choose-and-create-the-ml-model}{%
\section{STEP 3: Choose and create the ML
model}\label{step-3-choose-and-create-the-ml-model}}

After exploring the dataset, in this step one has to decide on a
specific ML algorithm based on certain selection criteria.

However, since the AI or ML world is so huge and impossible for a ML
novice to overlook, a brief description of the \textbf{relationship
between AI and ML} is given in the following sections. Furthermore, a
\textbf{taxonomy} of the different \textbf{learning types} is presented
by also providing some example algorithms.

\hypertarget{short-overview-of-the-ai-world-and-its-ml-algorithms}{%
\subsection{Short overview of the AI world and its ML
algorithms}\label{short-overview-of-the-ai-world-and-its-ml-algorithms}}

\hypertarget{relationship-between-ai-ml-and-others}{%
\subsubsection{Relationship between AI, ML and
others}\label{relationship-between-ai-ml-and-others}}

\textbf{@TODO:} Include in this section the presentation
\texttt{IFA\_Steimers\_KI\_Grundlagen\_Neuronaler\_Netze\_2021-03-22.pdf}
by Prof.~Steimers (IFA): - slides 6-7 ``Definition of KI'' - slides
10-12 ``ML: Categories based on the data, task and algorithms''

In the \textbf{science world}, the term \textbf{artificial intelligence
(AI)} refers to machines and systems that are capable of performing
tasks that are characteristic of human intelligence.

In the \textbf{business world}, on the other hand, AI typically refers
to mechanisms that perceive environmental factors and take autonomous
actions. This is seen as an opportunity to achieve \textbf{predefined
goals} with maximum success - without human intervention. Ultimately,
this view is a mapping of \textbf{input information} to controlled
\textbf{output actions} of a system. This expectation of AI-driven
systems is thus hardly higher than what can be expected from today's
modern automation systems.

\textbf{Machine Learning (ML)}, on the other hand, addresses the
mathematical models and algorithms that enable a computer system to
recognize (new) correlations in huge amounts of sample data from various
sources by inferring them independently. For scientists, machine
learning is a subset of AI.

The umbrella term AI covers a very large research area. It includes a
number of techniques that enable computers to learn independently and
solve complex problems:

\begin{itemize}
\tightlist
\item
  Computer-Vision (CV)
\item
  Supervised and Unsupervised Learning
\item
  Reinforcement Learning and Genetic Algorithms
\item
  Computational Linguistics
\item
  Robotics
\item
  etc.
\end{itemize}

The following Venn diagram shows the relationship between Artificial
Intelligence (AI), Machine Learning (ML) and other integrated
technologies. The quantities that do not belong to the main category
represent techniques that can function as stand-alone techniques and do
not necessarily fall into the artificial intelligence group in all cases
(for further details see
\href{https://www.researchgate.net/publication/336375517_Emerging_technologies_based_on_artificial_intelligence_to_assess_quality_and_consumer_preference_of_beverages}{Emerging
technologies based on artificial intelligence to assess quality and
consumer preference of beverages}).

    \begin{figure}
\centering
\includegraphics{images/AI_ML_venn_diagram_wide.png}
\caption{Venn diagram showing the relationship between Artificial
Intelligence (AI), Machine Learning (ML) and other integrated
technologies (source: Kasper, adapted from
\href{https://www.researchgate.net/publication/336375517_Emerging_technologies_based_on_artificial_intelligence_to_assess_quality_and_consumer_preference_of_beverages}{Emerging
technologies based on artificial intelligence to assess quality and
consumer preference of beverages}, license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{taxonomy-of-machine-learning}{%
\subsubsection{Taxonomy of machine
learning}\label{taxonomy-of-machine-learning}}

The field of machine learning can be divided into the following
\textbf{types of learning}:

\begin{itemize}
\tightlist
\item
  Supervised learning
\item
  Unsupervised learning
\item
  Semi-supervised learning
\item
  Reinforcement learning
\end{itemize}

Here are some further sources:

\begin{itemize}
\tightlist
\item
  \href{https://subscription.packtpub.com/book/big-data-/9781783558742/1/ch01lvl1sec12/taxonomy-of-machine-learning-algorithms}{Taxonomy
  of machine learning algorithms}
\item
  \href{https://www.researchgate.net/publication/340878018_Comprehensive_Survey_of_Machine_Learning_Approaches_in_Cognitive_Radio-Based_Vehicular_Ad_Hoc_Networks}{Comprehensive
  Survey of Machine Learning Approaches in Cognitive Radio-Based
  Vehicular Ad Hoc Networks}
\item
  \href{https://www.researchgate.net/publication/358089496_A_Taxonomy_of_Machine_Learning_Techniques}{A
  Taxonomy of Machine Learning Techniques}
\item
  \href{https://medium.com/@Shaier/ml-algorithms-one-sd-\%CF\%83-74bcb28fafb6}{ML
  Algorithms: One SD}
\item
  \href{https://github.com/trekhleb/homemade-machine-learning\#machine-learning-map}{Machine
  Learning Map}
\end{itemize}

    \hypertarget{supervised-learning}{%
\paragraph{Supervised learning}\label{supervised-learning}}

The goal of \textbf{supervised learning (SL)} is to learn a
\textbf{function} that maps a \textbf{input to an output}, based on
example input-output pairs. This involves inferring a relationship
describable by a mathematical function from \textbf{labeled training
data} consisting of a set of training examples (see
\href{https://en.wikipedia.org/wiki/Supervised_learning}{Supervised
Learning}).

A few well-known algorithms from the field of \textbf{supervised
learning} are mentioned here:

\begin{itemize}
\tightlist
\item
  Naive Bayes
\item
  Linear Regression
\item
  Logistic Regression
\item
  Artificial Neural Networks (ANN)
\item
  Support Vector Classifier (SVC)
\item
  Decision Trees
\item
  Random Forests
\end{itemize}

    \hypertarget{unsupervised-learning}{%
\paragraph{Unsupervised learning}\label{unsupervised-learning}}

The algorithms of this category look for internal structures in the data
of a dataset, such as \textbf{grouping} or \textbf{clustering of data
points}. These algorithms can thus learn relationships from test data
that have not been labeled, classified, or categorized. Rather than
responding to feedback (as in supervised learning), unsupervised
learning algorithms detect \textbf{commonalities in the data} and
respond based on the presence or absence of such commonalities in each
new dataset (see
\href{https://en.wikipedia.org/wiki/Machine_learning\#Unsupervised_learning}{Unsupervised
learning}).

Here are some algorithms from the field of \textbf{unsupervised
learning}:

\begin{itemize}
\tightlist
\item
  K-means Clustering
\item
  Spectral Clustering
\item
  Hierarchical Clustering
\item
  Principal Component Analysis (PCA)
\end{itemize}

    \hypertarget{semi-supervised-learning}{%
\paragraph{Semi-supervised learning}\label{semi-supervised-learning}}

This type of learning falls between \textbf{unsupervised} learning
(without any labeled training data) and \textbf{supervised} learning
(with completely labeled training data). Some of the training examples
are missing training labels, yet many machine-learning researchers have
found that unlabeled data, when used in conjunction with a small amount
of labeled data, can produce a considerable improvement in learning
accuracy (source:
\href{https://en.wikipedia.org/wiki/Machine_learning\#Semi-supervised_learning}{Semi-supervised
learning}).

    \hypertarget{reinforcement-learning}{%
\paragraph{Reinforcement learning}\label{reinforcement-learning}}

This is an area of machine learning concerned with how
\textbf{intelligent agents} ought to \textbf{take actions in an
environment} in order to maximize the notion of cumulative
\textbf{reward}. Due to its generality, the field is studied in many
other disciplines, such as \textbf{game theory} and \textbf{control
theory}.

Reinforcement learning differs from supervised learning in \textbf{not
needing labeled input/output pairs} be presented, and in not needing
sub-optimal actions to be explicitly corrected. Instead the focus is on
\textbf{finding a balance} between \textbf{exploration} (of uncharted
territory) and \textbf{exploitation} (of current knowledge) (source:
\href{https://en.wikipedia.org/wiki/Reinforcement_learning}{Reinforcement
learning}).

Here are some algorithms from the field of \textbf{reinforcement
learning}:

\begin{itemize}
\tightlist
\item
  Iterative Policy
\item
  Q-Learning
\item
  SARSA
\item
  Learning Classifiers
\item
  Stochastic Gradient
\item
  Genetic Algorithm
\end{itemize}

    \hypertarget{decision-graph-for-selecting-an-suitable-algorithm}{%
\subsection{Decision graph for selecting an suitable
algorithm}\label{decision-graph-for-selecting-an-suitable-algorithm}}

Now that the iris dataset has been analyzed in terms of its data
structure and internal correlations, the most difficult task on the way
to solving a problem using machine learning arises: finding the
``right'' ML algorithm (also called \textbf{estimator}).

The diverse estimators available are more or less well qualified for the
respective problems with their partly very different data types. The
good news is that the ML software package \textbf{Scikit-Learn} provides
the following \textbf{flowchart} as a rough \textbf{guide} in choosing
the right estimator for the particular task (see:
\href{https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html}{Choosing
the right estimator}).

However, it must also be emphasized that a considerable \textbf{level of
experience} through systematic trial and error is crucial to be
successful in finding an ``optimal'' estimator.

    \begin{figure}
\centering
\includegraphics{images/scikit-learn_ml_algorithm_decision.png}
\caption{Decision graph for choosing an appropriate ML algorithm
(source:
\href{https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html}{Choosing
the right estimator}, license: unknown)}
\end{figure}

    \hypertarget{reasons-for-choosing-support-vector-classifier-svc}{%
\subsection{Reasons for choosing Support Vector Classifier
(SVC)}\label{reasons-for-choosing-support-vector-classifier-svc}}

Among other ML algorithms suitable for the Iris dataset (such as the
decision-tree-based \textbf{random-forests classifier}), the reasoned
choice here in this tutorial falls on the \textbf{support vector
classifier (SVC)}.

The following \textbf{reasons} led to the decision for the
\textbf{Support Vector Classifier (SVC)}:

\begin{itemize}
\tightlist
\item
  the aim is to predict the species using unlabeled test data, so the
  task is to \textbf{classify}
\item
  the iris dataset is \textbf{fully labeled} (by designating the iris
  species)
\item
  the dataset contains significantly \textbf{less than 100k samples}
\end{itemize}

But the most important reason is that it is \textbf{easy to understand}
how it works - so it is exactly suitable for a beginner tutorial ;)

    \hypertarget{operating-principal-of-svc}{%
\subsection{Operating principal of
SVC}\label{operating-principal-of-svc}}

\begin{quote}
Support Vector Classifiers (SVC) try to \textbf{find the best hyperplane
to separate} the different classes by maximizing the distance between
sample points and the hyperplane (source:
\href{https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769}{In
Depth: Parameter tuning for SVC}).
\end{quote}

The following figure shows the operating principal of the SVC algorithm:
the hyperplanes \emph{H1} till \emph{H4} (left graphic) do separate the
classes. A good separation is achieved by the hyperplane that has the
largest distance to the nearest training-data point of any class
(so-called functional margin), since in general the larger the margin,
the lower the generalization error of the classifier (source:
\href{https://en.wikipedia.org/wiki/Support-vector_machine}{Support-vector
machine}).

The right graphic shows the optimal hyperplane characterized by
maximizing the margin between the classes. The perpendicular distance of
the closest data points to the hyperplane determines their position and
orientation. These perpendicular distances are the \textbf{support
vectors} of the hyperplane - this is how the algorithm got its name.

    \begin{figure}
\centering
\includegraphics{images/SVC_operatingPrinciple.png}
\caption{Support Vector Classifiers (SVC) separate the data points in
classes by finding the best hyperplane by maximizing the margin to its
support vectors (source: Kasper, license: CC BY-SA 4.0)}
\end{figure}

    \hypertarget{create-the-svc-model}{%
\subsection{Create the SVC model}\label{create-the-svc-model}}

In this step we create the SVC model choosing a \textbf{linear kernel}
with default parameters.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}
\PY{n}{classifier} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{step-4-prepare-the-dataset-for-training}{%
\section{STEP 4: Prepare the dataset for
training}\label{step-4-prepare-the-dataset-for-training}}

In this step the dataset is prepared for the actual classification by
SVC. Depending on the selected ML algorithm as well as the data
structure, it may be necessary to prepare the data before training
(e.g., by \textbf{standardization}, \textbf{normalization}, or
\textbf{binarization} based on thresholds). Furthermore, errors in the
dataset (e.g.~\textbf{data gaps}, \textbf{duplicates} or obvious
\textbf{misentries}) should be corrected now at the latest.

Through the intensive exploration of the data in
(\hyperref[step-2-explore-the-ml-dataset]{STEP 2: Explore the ML dataset}),
we know that special \textbf{preparation} of the data is \textbf{not
necessary}. The values are complete and without gaps and there are no
duplicates. The values are in similar ranges, which \textbf{does not
require normalization} of the data.

Furthermore, we know that the \textbf{classes} are very \textbf{evenly
distributed} and thus bias tendencies should be avoided.

For further details about \textbf{Standarization} and
\textbf{Normalization} read here:
\href{http://techflare.blog/what-are-standarization-and-normalization-test-with-iris-data-set-in-scikit-learn/}{What
are standarization and normalization? Test with iris data set in
Scikit-learn}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} import Iris dataset for exploration (again)}
\PY{n}{irisdata\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./datasets/IRIS\PYZus{}flower\PYZus{}dataset\PYZus{}kaggle.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{standarization}{%
\subsection{Standarization}\label{standarization}}

Standardize the feature values by computing the \textbf{mean},
subtracting the mean from the data points, and then dividing by the
\textbf{standard deviation}.

\textbf{@TODO:}\\
Incorporate section ``Skalieren von Merkmalen'' of the book
\texttt{OReilly\_Praxiseinstieg\_Machine\_Learning\_Scikit-Learn\_TensorFlow\_2018\_Anm\_bk.pdf}
(see \cite{Geron_2018}).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}

\PY{c+c1}{\PYZsh{}scaler = StandardScaler()}
\PY{c+c1}{\PYZsh{}X\PYZus{}train = scaler.fit\PYZus{}transform(X\PYZus{}train)}
\PY{c+c1}{\PYZsh{}X\PYZus{}test = scaler.transform(X\PYZus{}test)}
\PY{n}{irisdata\PYZus{}df}

\PY{c+c1}{\PYZsh{}X\PYZus{}train}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
     sepal\_length  sepal\_width  petal\_length  petal\_width         species
0             5.1          3.5           1.4          0.2     Iris-setosa
1             4.9          3.0           1.4          0.2     Iris-setosa
2             4.7          3.2           1.3          0.2     Iris-setosa
3             4.6          3.1           1.5          0.2     Iris-setosa
4             5.0          3.6           1.4          0.2     Iris-setosa
..            {\ldots}          {\ldots}           {\ldots}          {\ldots}             {\ldots}
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica

[150 rows x 5 columns]
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{normalization}{%
\subsection{Normalization}\label{normalization}}

    \hypertarget{step-5-carry-out-training-prediction-and-testing}{%
\section{STEP 5: Carry out training, prediction and
testing}\label{step-5-carry-out-training-prediction-and-testing}}

\hypertarget{split-the-dataset}{%
\subsection{Split the dataset}\label{split-the-dataset}}

In the next very important step, the dataset is split into \textbf{2
subsets}: a \textbf{training dataset} and a \textbf{test dataset}. As
the names suggest, the training dataset is used to train the ML
algorithm. The test dataset is then used to check the quality of the
trained ML algorithm (here the \textbf{recognition rate}). For this
purpose, the \textbf{class labels} are \textbf{removed} from the
training dataset - after all, these are to be predicted.

Typically, the \textbf{test dataset} should contain about \textbf{20\%}
of the entire dataset.

In particular, to \textbf{avoid bias} in the sorted iris dataset due to
splitting, the \textbf{order} of the data rows must be
\textbf{randomized}. This is done with the parameter
\texttt{shuffle=True}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}

\PY{n}{X} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.20}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Check that the split datasets are still balanced and that no
\textbf{bias} has been created by the splitting.

For this test, the previously separated labels \texttt{y\_train} must be
added back to the training dataset \texttt{X\_train}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} make a deep copy of \PYZsq{}X\PYZus{}train\PYZsq{}}
\PY{n}{X\PYZus{}train\PYZus{}bias\PYZus{}test\PYZus{}df} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} add list of labels to test dataframe}
\PY{n}{X\PYZus{}train\PYZus{}bias\PYZus{}test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}train}

\PY{c+c1}{\PYZsh{} count unique values without missing values in a column, }
\PY{c+c1}{\PYZsh{} ordered descending and normalized}
\PY{n}{X\PYZus{}train\PYZus{}bias\PYZus{}test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{dropna}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}, frame=single, framerule=2mm, rulecolor=\color{outerrorbackground}]
\textcolor{ansi-red}{---------------------------------------------------------------------------}
\textcolor{ansi-red}{TypeError}                                 Traceback (most recent call last)
Input \textcolor{ansi-green}{In [37]}, in \textcolor{ansi-cyan}{<cell line: 2>}\textcolor{ansi-blue}{()}
\textcolor{ansi-green-intense}{\textbf{      1}} \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{95,135,135}}{\# make a deep copy of 'X\_train'}
\textcolor{ansi-green}{----> 2} X\_train\_bias\_test\_df \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{98,98,98}}{=} \setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{X\_train\strut}\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{98,98,98}}{\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{.\strut}}\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{copy\strut}\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{(\strut}\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{deep\strut}\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{98,98,98}}{\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{=\strut}}\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,0}}{\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{\textbf{True}\strut}}\setlength{\fboxsep}{0pt}\colorbox{ansi-yellow}{)\strut}
\textcolor{ansi-green-intense}{\textbf{      4}} \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{95,135,135}}{\# add list of labels to test dataframe}
\textcolor{ansi-green-intense}{\textbf{      5}} X\_train\_bias\_test\_df[\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{175,0,0}}{'}\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{175,0,0}}{species}\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{175,0,0}}{'}] \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{98,98,98}}{=} y\_train

\textcolor{ansi-red}{TypeError}: copy() got an unexpected keyword argument 'deep'
    \end{Verbatim}

    For training, do not use only the variables that correlate best with
each other, but all of them.

Otherwise, the result of the prediction would be significantly worse.
Maybe this is already an indication of \textbf{overfitting} of the ML
model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} DO NOT USE THIS!!}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                                       \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} 
                                                    \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.20}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{train-the-svc}{%
\subsection{Train the SVC}\label{train-the-svc}}

In this step the SVC is trained with the training data. Training means
to \textbf{fit} the SVC to the \textbf{training data}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} fit the model for the data}
\PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
SVC(kernel='linear', random\_state=0)
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{make-predictions}{%
\subsection{Make predictions}\label{make-predictions}}

In this step the aim is to \textbf{predict the species} using unlabeled
test data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{c+c1}{\PYZsh{}X\PYZus{}test}
\PY{c+c1}{\PYZsh{}y\PYZus{}pred}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{step-6-evaluate-models-performance}{%
\section{STEP 6: Evaluate model's
performance}\label{step-6-evaluate-models-performance}}

Subsequently to the training of the SVC model and the classification
predictions made based on the test data, this step evaluates the
\textbf{quality of the classification result} using known
\textbf{metrics} such as the \textbf{accuracy score} as well as the
\textbf{confusion matrix}.

    \hypertarget{accuracy-score}{%
\subsection{Accuracy Score}\label{accuracy-score}}

In a multilabel classification (such as the Iris dataset), this
\textbf{Accuracy classification score} computes the subset accuracy. For
further details see
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\#sklearn.metrics.accuracy_score}{sklearn.metrics.accuracy\_score}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}

\PY{n}{acc\PYZus{}score} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{acc\PYZus{}score}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy score: 80.00 \%
    \end{Verbatim}

    \hypertarget{classification-report}{%
\subsection{Classification Report}\label{classification-report}}

The classification report shows a representation of the main
\textbf{classification metrics on a per-class basis}. This gives a
deeper intuition of the classifier behavior over global accuracy which
can mask functional weaknesses in one class of a multiclass problem (see
\href{https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html}{Classification
Report}).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00         5
Iris-versicolor       0.86      0.75      0.80        16
 Iris-virginica       0.64      0.78      0.70         9

       accuracy                           0.80        30
      macro avg       0.83      0.84      0.83        30
   weighted avg       0.81      0.80      0.80        30

    \end{Verbatim}

    \hypertarget{cross-validation-score}{%
\subsection{Cross-validation score}\label{cross-validation-score}}

The function \texttt{cross\_val\_score()} from the Scikit-learn package
\textbf{trains and tests a model over multiple folds} of your dataset.
This cross validation method gives a better \textbf{understanding of
model performance} over the whole dataset instead of just a single
train/test split (see
\href{https://stephenallwright.com/cross_val_score-sklearn/}{Using
cross\_val\_score in sklearn, simply explained}).

\textbf{@TODO:}\\
Incorporate section ``Bessere Auswertung mittels Kreuzvalidierung'' of
the book
\texttt{OReilly\_Praxiseinstieg\_Machine\_Learning\_Scikit-Learn\_TensorFlow\_2018\_Anm\_bk.pdf}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}

\PY{n}{accuracies} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{classifier}\PY{p}{,} \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{,} 
                             \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard Deviation: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cross-validation score: 82.50 \%
Standard Deviation: 14.65 \%
    \end{Verbatim}

    \hypertarget{confusion-matrix}{%
\subsection{Confusion matrix}\label{confusion-matrix}}

The \textbf{confusion matrix} measures the quality of predictions from a
classification model by looking at how many \textbf{predictions} are
\textbf{True} and how many are \textbf{False} (see
\href{https://www.jcchouinard.com/confusion-matrix-in-scikit-learn/}{What
the Confusion Matrix Measures?}.

\hypertarget{textual-confusion-matrix}{%
\subsubsection{Textual confusion
matrix}\label{textual-confusion-matrix}}

For checking the accuracy of the model, the \textbf{confusion matrix}
can be used for the \textbf{cross validation}.

By using the function \texttt{sklearn.metrics.confusion\_matrix()} a
confusion matrix of the true iris class labels versus the predicted
class labels is plotted.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cm} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cm}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[ 6  0  0]
 [ 0  9  1]
 [ 0  0 14]]
    \end{Verbatim}

    \hypertarget{colored-confusion-matrix}{%
\subsubsection{Colored confusion
matrix}\label{colored-confusion-matrix}}

The function \texttt{sklearn.metrics.ConfusionMatrixDisplay()} plots a
colored confusion matrix.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print colored confusion matrix}
\PY{n}{cm\PYZus{}colored} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}predictions}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Colored Confusion Matrix}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figwidth}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figheight}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{confusion\PYZus{}matrix}

\PY{c+c1}{\PYZsh{} save figure as PNG}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/confusion\PYZus{}matrix.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{,} \PY{n}{pad\PYZus{}inches}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_148_0.png}\end{center}
        \caption{Checking the accuracy of the model by using the confusion matrix for cross-validation}
        \label{fig:confusion_matrix}
    \end{figure}
    
    \hypertarget{step-7-vary-parameters-of-the-ml-model-manually}{%
\section{STEP 7: Vary parameters of the ML model
manually}\label{step-7-vary-parameters-of-the-ml-model-manually}}

This section was inspired by
\href{https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769}{In
Depth: Parameter tuning for SVC}

In this section, the 4 SVC parameters \texttt{kernel}, \texttt{gamma},
\texttt{C} and \texttt{degree} will be introduced one by one.
Furthermore, their influence on the classification result by varying
these single parameters will be shown.

\textbf{Disclaimer:} In order to show the effects of varying the
individual parameters in 2D graphs, only the best correlating variables
\texttt{petal\_length} and \texttt{petal\_width} are used to train the
SVC.

\hypertarget{prepare-dataset}{%
\subsection{Prepare dataset}\label{prepare-dataset}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} import iris dataset again}
\PY{n}{irisdata\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./datasets/IRIS\PYZus{}flower\PYZus{}dataset\PYZus{}kaggle.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} encode the class column from class strings to integer equivalents}
\PY{n}{irisdata\PYZus{}df\PYZus{}enc} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{species}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}  \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iris\PYZhy{}setosa}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{0}\PY{p}{,}
                                                    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iris\PYZhy{}versicolor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{,} 
                                                    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iris\PYZhy{}virginica}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{\PYZcb{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}irisdata\PYZus{}df\PYZus{}enc}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{prepare-datasets-for-parameter-variation-and-plotting}{%
\subsubsection{Prepare datasets for parameter variation and
plotting}\label{prepare-datasets-for-parameter-variation-and-plotting}}

These datasets will be used for parameter variation and plotting only.
In particular, for later \textbf{2D plotting} of the effects of
parameter variation, only \textbf{2 variables} of the iris dataset can
be used.

However, as seen in the previous section, this selection is very much at
the expense of detection accuracy. Therefore, it is not useful to make
predictions with this subset of data - it is not necessary to divide it
into a training and a test dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} copy only 2 feature columns}
\PY{c+c1}{\PYZsh{} and convert pandas dataframe to numpy array}
\PY{n}{X\PYZus{}plot} \PY{o}{=} \PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{n}{copy}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{c+c1}{\PYZsh{}X\PYZus{}plot = irisdata\PYZus{}df\PYZus{}enc[[\PYZsq{}sepal\PYZus{}length\PYZsq{}, \PYZsq{}sepal\PYZus{}width\PYZsq{}]].to\PYZus{}numpy(copy=True)}
\PY{c+c1}{\PYZsh{}X\PYZus{}plot}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} convert pandas dataframe to numpy array}
\PY{c+c1}{\PYZsh{} and get a flat 1D copy of 2D numpy array}
\PY{n}{y\PYZus{}plot} \PY{o}{=} \PY{n}{irisdata\PYZus{}df\PYZus{}enc}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{n}{copy}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}y\PYZus{}plot}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{prepare-dataset-for-prediction-and-evaluation}{%
\subsubsection{Prepare dataset for prediction and
evaluation}\label{prepare-dataset-for-prediction-and-evaluation}}

To \textbf{evaluate the recognition accuracy} by parameter variation,
the complete iris dataset with all variables must be used. To make
predictions with test data, the dataset is again divided into a training
and a test dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.20}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{plotting-functions}{%
\subsection{Plotting functions}\label{plotting-functions}}

This function helps to visualize the modifications by varying the
individual SVC parameters:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plotSVC}\PY{p}{(}\PY{n}{title}\PY{p}{,} \PY{n}{svc}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{xlabel}\PY{p}{,} \PY{n}{ylabel}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} create a mesh to plot in}
    \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    
    \PY{c+c1}{\PYZsh{} prevent division by zero}
    \PY{k}{if} \PY{n}{x\PYZus{}min} \PY{o}{==} \PY{l+m+mf}{0.0}\PY{p}{:}
        \PY{n}{x\PYZus{}min} \PY{o}{=} \PY{l+m+mf}{0.1}
    
    \PY{n}{h} \PY{o}{=} \PY{p}{(}\PY{n}{x\PYZus{}max} \PY{o}{/} \PY{n}{x\PYZus{}min}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{1000}
    \PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{)}
    
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{n}{svc}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{xx}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
    
    \PY{n}{plt}\PY{o}{.}\PY{n}{contourf}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{Paired}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.6}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{Paired}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{xlabel}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{n}{ylabel}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{xx}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{title}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    This function cares for cross validation:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{scale}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} train the SVC}
    \PY{n}{svc} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{n}{kernel}\PY{p}{,} 
                  \PY{n}{gamma}\PY{o}{=}\PY{n}{gamma}\PY{p}{,} 
                  \PY{n}{C}\PY{o}{=}\PY{n}{C}\PY{p}{,} 
                  \PY{n}{degree}\PY{o}{=}\PY{n}{degree}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} calculate accuracies}
    \PY{n}{accuracies} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{svc}\PY{p}{,} \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{,} 
                                 \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
    
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{accuracies}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}
    \PY{k}{return} \PY{n}{accuracy}
\end{Verbatim}
\end{tcolorbox}

    This function plots the variation of the SVC parameters against the
prediction accuracy to show the effect of variation and its limits
regarding the phenomenon \textbf{overfitting}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{plotParamsAcc}\PY{p}{(}\PY{n}{param\PYZus{}list}\PY{p}{,} \PY{n}{acc\PYZus{}list}\PY{p}{,} \PY{n}{param\PYZus{}name}\PY{p}{,} \PY{n}{log\PYZus{}scale}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
    \PY{n}{title\PYZus{}str} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variation of }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ parameter }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{param\PYZus{}name}\PY{p}{)} \PYZbs{}
                \PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{and its effect to prediction accuracy}\PY{l+s+s1}{\PYZsq{}}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{title\PYZus{}str}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{param\PYZus{}list}\PY{p}{,} \PY{n}{accuracy\PYZus{}list}\PY{p}{)}
    \PY{k}{if} \PY{n}{log\PYZus{}scale}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} set the X axis scale to logarithmic}
        \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{param\PYZus{}name}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy [}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{vary-kernel-of-svc}{%
\subsection{\texorpdfstring{Vary \texttt{kernel} of
SVC}{Vary kernel of SVC}}\label{vary-kernel-of-svc}}

The \texttt{kernel} parameter selects the type of hyperplane that is
used to separate the data. Using \texttt{linear}
(\href{https://en.wikipedia.org/wiki/Linear_classifier}{linear
classifier}) kernel will use a linear hyperplane (a line in the case of
2D data). The \texttt{rbf}
(\href{https://en.wikipedia.org/wiki/Radial_basis_function_kernel}{radial
basis function kernel}) and \texttt{poly}
(\href{https://en.wikipedia.org/wiki/Polynomial_kernel}{polynomial
kernel}) kernel use non linear hyperplanes. The \textbf{default} is
\texttt{kernel=rbf}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{kernels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{poly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{xlabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal length}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{ylabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal width}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{kernel} \PY{o+ow}{in} \PY{n}{kernels}\PY{p}{:}
    \PY{n}{svc\PYZus{}plot} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{n}{kernel}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{)}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{n}{kernel}\PY{p}{)}
    \PY{n}{title\PYZus{}str} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel: }\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{kernel}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{, }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Acc. prediction: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{n}{plotSVC}\PY{p}{(}\PY{n}{title\PYZus{}str}\PY{p}{,} \PY{n}{svc\PYZus{}plot}\PY{p}{,} \PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{,} \PY{n}{xlabel}\PY{p}{,} \PY{n}{ylabel}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_163_0.png}\end{center}
        \caption{This group of images shows the effect on the classification by the choice of the different SVC kernels ('linear', 'rbf', 'poly' and 'sigmoid')}
        \label{fig:vary_kernels}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_163_1.png}\end{center}
        \caption{This group of images shows the effect on the classification by the choice of the different SVC kernels ('linear', 'rbf', 'poly' and 'sigmoid')}
        \label{fig:vary_kernels}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_163_2.png}\end{center}
        \caption{This group of images shows the effect on the classification by the choice of the different SVC kernels ('linear', 'rbf', 'poly' and 'sigmoid')}
        \label{fig:vary_kernels}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_163_3.png}\end{center}
        \caption{This group of images shows the effect on the classification by the choice of the different SVC kernels ('linear', 'rbf', 'poly' and 'sigmoid')}
        \label{fig:vary_kernels}
    \end{figure}
    
    \hypertarget{vary-gamma-parameter}{%
\subsection{\texorpdfstring{Vary \texttt{gamma}
parameter}{Vary gamma parameter}}\label{vary-gamma-parameter}}

The \texttt{gamma} parameter is used for \textbf{non linear
hyperplanes}. The higher the \texttt{gamma} float value it tries to
exactly fit the training dataset. The \textbf{default} is
\texttt{gamma=\textquotesingle{}scale\textquotesingle{}}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{gammas} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}

\PY{n}{xlabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal length}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{ylabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal width}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{gamma} \PY{o+ow}{in} \PY{n}{gammas}\PY{p}{:}
    \PY{n}{svc\PYZus{}plot} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{n}{gamma}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{)}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{n}{gamma}\PY{p}{)}
    \PY{n}{title\PYZus{}str} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma: }\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{gamma}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{, }\PY{l+s+s1}{\PYZsq{}} \PYZbs{}
                \PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Acc. prediction: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{n}{plotSVC}\PY{p}{(}\PY{n}{title\PYZus{}str}\PY{p}{,} \PY{n}{svc\PYZus{}plot}\PY{p}{,} \PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{,} \PY{n}{xlabel}\PY{p}{,} \PY{n}{ylabel}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_165_0.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'gamma' of the 'rbf' kernel}
        \label{fig:vary_gamma_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_165_1.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'gamma' of the 'rbf' kernel}
        \label{fig:vary_gamma_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_165_2.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'gamma' of the 'rbf' kernel}
        \label{fig:vary_gamma_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_165_3.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'gamma' of the 'rbf' kernel}
        \label{fig:vary_gamma_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_165_4.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'gamma' of the 'rbf' kernel}
        \label{fig:vary_gamma_parameter}
    \end{figure}
    
    Show the variation of the SVC parameter \texttt{gamma} against the
\textbf{prediction accuracy}.

As we can see, increasing \texttt{gamma} leads to \textbf{overfitting}
as the classifier tries to perfectly fit the training data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{gammas} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}

\PY{n}{accuracy\PYZus{}list} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{gamma} \PY{o+ow}{in} \PY{n}{gammas}\PY{p}{:}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{n}{gamma}\PY{p}{)}
    \PY{n}{accuracy\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}

\PY{n}{plotParamsAcc}\PY{p}{(}\PY{n}{gammas}\PY{p}{,} \PY{n}{accuracy\PYZus{}list}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{log\PYZus{}scale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_167_0.png}\end{center}
        \caption{The plot shows the variation of the SVC parameter 'gamma' against the prediction accuracy}
        \label{fig:plot_vary_gamma}
    \end{figure}
    
    \hypertarget{vary-c-parameter}{%
\subsection{\texorpdfstring{Vary \texttt{C}
parameter}{Vary C parameter}}\label{vary-c-parameter}}

The \texttt{C} parameter is the \textbf{penalty} of the error term. It
controls the trade off between smooth decision boundary and classifying
the training points correctly. The \textbf{default} is \texttt{C=1.0}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cs} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{]}

\PY{n}{xlabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal length}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{ylabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal width}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{cs}\PY{p}{:}
    \PY{n}{svc\PYZus{}plot} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{c}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{)}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{c}\PY{p}{)}
    \PY{n}{title\PYZus{}str} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C: }\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{c}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{, }\PY{l+s+s1}{\PYZsq{}} \PYZbs{}
                 \PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Acc. prediction: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{n}{plotSVC}\PY{p}{(}\PY{n}{title\PYZus{}str}\PY{p}{,} \PY{n}{svc\PYZus{}plot}\PY{p}{,} \PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{,} \PY{n}{xlabel}\PY{p}{,} \PY{n}{ylabel}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_0.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_1.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_2.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_3.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_4.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_5.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_169_6.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'C' of the 'rbf' kernel}
        \label{fig:vary_c_parameter}
    \end{figure}
    
    Show the variation of the SVC parameter \texttt{C} against the
\textbf{prediction accuracy}.

But be careful: to high \texttt{C} values may lead to
\textbf{overfitting} the training data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cs} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{]}

\PY{n}{accuracy\PYZus{}list} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n}{cs}\PY{p}{:}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{c}\PY{p}{)}
    \PY{n}{accuracy\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}

\PY{n}{plotParamsAcc}\PY{p}{(}\PY{n}{cs}\PY{p}{,} \PY{n}{accuracy\PYZus{}list}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{log\PYZus{}scale}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_171_0.png}\end{center}
        \caption{The plot shows the variation of the SVC parameter 'C' against the prediction accuracy}
        \label{fig:plot_vary_c}
    \end{figure}
    
    \hypertarget{vary-degree-parameter}{%
\subsection{\texorpdfstring{Vary \texttt{degree}
parameter}{Vary degree parameter}}\label{vary-degree-parameter}}

The \texttt{degree} parameter is used when the \texttt{kernel} is set to
\texttt{poly} and is ignored by all other kernels. It's basically the
\textbf{degree of the polynomial} used to find the hyperplane to split
the data. The \textbf{default} is \texttt{degree=3}.

Using \texttt{degree\ =\ 1} is the same as using a \texttt{linear}
kernel. Also, increasing this parameters leads to \textbf{higher
training times}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{degrees} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}

\PY{n}{xlabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal length}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{ylabel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Petal width}\PY{l+s+s1}{\PYZsq{}}

\PY{k}{for} \PY{n}{degree} \PY{o+ow}{in} \PY{n}{degrees}\PY{p}{:}
    \PY{n}{svc\PYZus{}plot} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{poly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{n}{degree}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{)}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{poly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{n}{degree}\PY{p}{)}
    \PY{n}{title\PYZus{}str} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree: }\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{degree}\PY{p}{)}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{, }\PY{l+s+s1}{\PYZsq{}} \PYZbs{}
                 \PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Acc. prediction: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}
    \PY{n}{plotSVC}\PY{p}{(}\PY{n}{title\PYZus{}str}\PY{p}{,} \PY{n}{svc\PYZus{}plot}\PY{p}{,} \PY{n}{X\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{,} \PY{n}{xlabel}\PY{p}{,} \PY{n}{ylabel}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_0.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_1.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_2.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_3.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_4.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_5.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_6.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_7.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_8.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_173_9.png}\end{center}
        \caption{This group of images shows the effect on the classification by the variation of the parameter 'degree' of the 'poly' kernel}
        \label{fig:vary_degree_parameter}
    \end{figure}
    
    Show the variation of the SVC parameter \texttt{degree} against the
\textbf{prediction accuracy}.

As we can see, increasing the \texttt{degree} of the polynomial
hyperplane leads to \textbf{overfitting} the training data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{degrees} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}

\PY{n}{accuracy\PYZus{}list} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
\PY{k}{for} \PY{n}{degree} \PY{o+ow}{in} \PY{n}{degrees}\PY{p}{:}
    \PY{n}{accuracy} \PY{o}{=} \PY{n}{crossValSVC}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{poly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{n}{degree}\PY{p}{)}
    \PY{n}{accuracy\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{accuracy}\PY{p}{)}

\PY{n}{plotParamsAcc}\PY{p}{(}\PY{n}{degrees}\PY{p}{,} \PY{n}{accuracy\PYZus{}list}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{log\PYZus{}scale}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\adjustimage{max size={0.9\linewidth}{0.4\paperheight}}{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_175_0.png}\end{center}
        \caption{The plot shows the variation of the SVC parameter 'degree' against the prediction accuracy}
        \label{fig:plot_vary_degree}
    \end{figure}
    
    \hypertarget{step-8-tune-the-ml-model-systematically}{%
\section{STEP 8: Tune the ML model
systematically}\label{step-8-tune-the-ml-model-systematically}}

In the final step, two approaches to systematic hyper-parameter search
are presented: \textbf{Grid Search} and \textbf{Randomized Search}.
While the former exhaustively considers all parameter combinations for
given values, the latter selects a number of candidates from a parameter
space with a particular random distribution.

Sources:

\begin{itemize}
\tightlist
\item
  \href{https://scikit-learn.org/stable/modules/grid_search.html}{3.2.
  Tuning the hyper-parameters of an estimator}

  \begin{itemize}
  \tightlist
  \item
    \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html}{sklearn.model\_selection.GridSearchCV}
  \item
    \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\#sklearn.model_selection.RandomizedSearchCV}{sklearn.model\_selection.RandomizedSearchCV}
  \end{itemize}
\item
  \href{https://pyimagesearch.com/2021/05/17/introduction-to-hyperparameter-tuning-with-scikit-learn-and-python/}{Introduction
  to hyperparameter tuning with scikit-learn and Python}

  \begin{itemize}
  \tightlist
  \item
    \href{https://www.kaggle.com/datasets/rodolfomendes/abalone-dataset?resource=download}{Abalone
    Dataset}
  \end{itemize}
\item
  \href{https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35}{Hyperparameter
  tuning using Grid Search and Random Search: A Conceptual Guide}
\end{itemize}

    Import the necessary packages:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} general packages}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}
\PY{c+c1}{\PYZsh{}from sklearn.svm import SVC}
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{svm}\PY{p}{,} \PY{n}{metrics}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline

\PY{c+c1}{\PYZsh{} additional packages for grid search}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{RepeatedKFold}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{GridSearchCV}

\PY{c+c1}{\PYZsh{} additional packages for randomized search}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{RandomizedSearchCV}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{RepeatedKFold}

\PY{c+c1}{\PYZsh{} import class MeasExecTimeOfProgram from python file MeasExecTimeOfProgramclass.py}
\PY{k+kn}{from} \PY{n+nn}{MeasExecTimeOfProgram\PYZus{}class} \PY{k+kn}{import} \PY{n}{MeasExecTimeOfProgram}
\end{Verbatim}
\end{tcolorbox}

    Set path and columns of the Iris dataset for import:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} specify the path of the dataset}
\PY{n}{CSV\PYZus{}PATH} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{./datasets/IRIS\PYZus{}flower\PYZus{}dataset\PYZus{}kaggle.csv}\PY{l+s+s2}{\PYZdq{}}
\end{Verbatim}
\end{tcolorbox}

    Load dataset and split it into subsets for training and testing in the
ratio 80\% to 20\%:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} load the dataset, separate the features and labels, and perform a}
\PY{c+c1}{\PYZsh{} training and testing split using 80\PYZpc{} of the data for training and}
\PY{c+c1}{\PYZsh{} 20\PYZpc{} for evaluation}
\PY{n}{irisdata\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{CSV\PYZus{}PATH}\PY{p}{)}

\PY{n}{X} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{irisdata\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.20}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Check that the split datasets are still balanced and that no
\textbf{bias} has been created by the splitting.

For this test, the previously separated labels \texttt{y\_train} must be
added back to the training dataset \texttt{X\_train}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} make a deep copy of \PYZsq{}X\PYZus{}train\PYZsq{}}
\PY{n}{X\PYZus{}train\PYZus{}bias\PYZus{}test\PYZus{}df} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} add list of labels to test dataframe}
\PY{n}{X\PYZus{}train\PYZus{}bias\PYZus{}test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y\PYZus{}train}

\PY{c+c1}{\PYZsh{} count unique values without missing values in a column, }
\PY{c+c1}{\PYZsh{} ordered descending and normalized}
\PY{n}{X\PYZus{}train\PYZus{}bias\PYZus{}test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{species}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{dropna}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Iris-versicolor    0.358333
Iris-virginica     0.333333
Iris-setosa        0.308333
Name: species, dtype: float64
\end{Verbatim}
\end{tcolorbox}
        
    Standardize the feature values by computing the \textbf{mean},
subtracting the mean from the data points, and then dividing by the
\textbf{standard deviation}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{}X\PYZus{}train}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{finding-a-baseline}{%
\subsection{Finding a baseline}\label{finding-a-baseline}}

The aim of this sub-step is to establish a baseline on the Iris dataset
by training a \textbf{Support Vector Classifier (SVC)} with no
hyperparameter tuning.

Train the model with \textbf{no tuning of hyperparameters} to find the
baseline for later improvements:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{classifier} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{kernel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} initiate measuring execution time}
\PY{n}{execTime} \PY{o}{=} \PY{n}{MeasExecTimeOfProgram}\PY{p}{(}\PY{p}{)}
\PY{n}{execTime}\PY{o}{.}\PY{n}{start}\PY{p}{(}\PY{p}{)}

\PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print time delta}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Execution time: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s1}{ ms}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{execTime}\PY{o}{.}\PY{n}{stop}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Execution time: 1.6954 ms
    \end{Verbatim}

    Evaluate our model using accuracy score:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} predict labels}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} calculate cross validation score}
\PY{c+c1}{\PYZsh{} HINT: do NOT use the accuracy score \PYZhy{} it\PYZsq{}s to inaccurate!}
\PY{n}{accuracies} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{classifier}\PY{p}{,} \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{,} 
                             \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard Deviation: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cross-validation score: 97.50 \%
Standard Deviation: 3.82 \%
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} print classification report}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        13
Iris-versicolor       1.00      0.86      0.92         7
 Iris-virginica       0.91      1.00      0.95        10

       accuracy                           0.97        30
      macro avg       0.97      0.95      0.96        30
   weighted avg       0.97      0.97      0.97        30

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print colored confusion matrix}
\PY{n}{cm\PYZus{}colored} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}predictions}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Colored Confusion Matrix}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figwidth}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figheight}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{confusion\PYZus{}matrix}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_193_0.png}\end{center}
        \caption{Confusion matrix for cross-validation of the baseline}
        \label{fig:cm_baseline}
    \end{figure}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{classifier}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'C': 1.0,
 'break\_ties': False,
 'cache\_size': 200,
 'class\_weight': None,
 'coef0': 0.0,
 'decision\_function\_shape': 'ovr',
 'degree': 3,
 'gamma': 'scale',
 'kernel': 'linear',
 'max\_iter': -1,
 'probability': False,
 'random\_state': 0,
 'shrinking': True,
 'tol': 0.001,
 'verbose': False\}
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{grid-search}{%
\subsection{Grid search}\label{grid-search}}

    Initialize the SVC model and define the \textbf{space of the
hyperparameters} to perform the \textbf{grid search} over:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{classifier} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{p}{)}

\PY{n}{kernels} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rbf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{poly}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{gammas} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}
\PY{n}{cs} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{]}

\PY{c+c1}{\PYZsh{} reduce the possible polynomial degrees to reasonable values,}
\PY{c+c1}{\PYZsh{} since with higher degrees the calculation time increases exponentially}
\PY{n}{degrees} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}

\PY{n}{grid} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{n}{kernels}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{n}{gammas}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{cs}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{n}{degrees}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Initialize a \textbf{cross-validation fold} and \textbf{perform a grid
search} to tune the hyperparameters:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cvFold} \PY{o}{=} \PY{n}{RepeatedKFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{n\PYZus{}repeats}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{gridSearch} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{estimator}\PY{o}{=}\PY{n}{classifier}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{grid}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
                          \PY{n}{cv}\PY{o}{=}\PY{n}{cvFold}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} initiate measuring execution time}
\PY{n}{execTime} \PY{o}{=} \PY{n}{MeasExecTimeOfProgram}\PY{p}{(}\PY{p}{)}
\PY{n}{execTime}\PY{o}{.}\PY{n}{start}\PY{p}{(}\PY{p}{)}

\PY{n}{searchResults} \PY{o}{=} \PY{n}{gridSearch}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print time delta}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Execution time: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{ s}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{execTime}\PY{o}{.}\PY{n}{stop}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Execution time: 39.64 s
    \end{Verbatim}

    Extract the best model and evaluate it:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} predict labels by best model}
\PY{n}{bestModel} \PY{o}{=} \PY{n}{searchResults}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}

\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{bestModel}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} calculate cross validation score from the best model}
\PY{c+c1}{\PYZsh{} HINT: do NOT use the accuracy score \PYZhy{} it\PYZsq{}s to inaccurate!}
\PY{n}{accuracies} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{bestModel}\PY{p}{,} \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{,} 
                             \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard Deviation: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cross-validation score: 98.33 \%
Standard Deviation: 3.33 \%
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        13
Iris-versicolor       1.00      0.86      0.92         7
 Iris-virginica       0.91      1.00      0.95        10

       accuracy                           0.97        30
      macro avg       0.97      0.95      0.96        30
   weighted avg       0.97      0.97      0.97        30

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print colored confusion matrix}
\PY{n}{cm\PYZus{}colored} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}predictions}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Colored Confusion Matrix}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figwidth}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figheight}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{confusion\PYZus{}matrix}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_204_0.png}\end{center}
        \caption{Confusion matrix for cross-validation after the grid search has been performed}
        \label{fig:cm_grid_search}
    \end{figure}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{bestModel}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'C': 5,
 'break\_ties': False,
 'cache\_size': 200,
 'class\_weight': None,
 'coef0': 0.0,
 'decision\_function\_shape': 'ovr',
 'degree': 1,
 'gamma': 0.1,
 'kernel': 'poly',
 'max\_iter': -1,
 'probability': False,
 'random\_state': None,
 'shrinking': True,
 'tol': 0.001,
 'verbose': False\}
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{randomized-search}{%
\subsection{Randomized search}\label{randomized-search}}

    Initialize the SVC model and define the \textbf{space of the
hyperparameters} to perform the \textbf{randomized search} over:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{72}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{classifier} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{p}{)}

\PY{n}{kernels} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rbf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigmoid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{poly}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{gammas} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}
\PY{n}{cs} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{]}

\PY{c+c1}{\PYZsh{} reduce the possible polynomial degrees to reasonable values,}
\PY{c+c1}{\PYZsh{} since with higher degrees the calculation time increases exponentially}
\PY{n}{degrees} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}

\PY{n}{grid} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{n}{kernels}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{n}{gammas}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{n}{cs}\PY{p}{,} \PY{n}{degree}\PY{o}{=}\PY{n}{degrees}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Initialize a \textbf{cross-validation fold} and \textbf{perform a
randomized search} to tune the hyperparameters:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cvFold} \PY{o}{=} \PY{n}{RepeatedKFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{n\PYZus{}repeats}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{randomSearch} \PY{o}{=} \PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{estimator}\PY{o}{=}\PY{n}{classifier}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
                                  \PY{n}{cv}\PY{o}{=}\PY{n}{cvFold}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{grid}\PY{p}{,}
                                  \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} initiate measuring execution time}
\PY{n}{execTime} \PY{o}{=} \PY{n}{MeasExecTimeOfProgram}\PY{p}{(}\PY{p}{)}
\PY{n}{execTime}\PY{o}{.}\PY{n}{start}\PY{p}{(}\PY{p}{)}

\PY{n}{searchResults} \PY{o}{=} \PY{n}{randomSearch}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print time delta}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Execution time: }\PY{l+s+si}{\PYZob{}:.3f\PYZcb{}}\PY{l+s+s1}{ s}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{execTime}\PY{o}{.}\PY{n}{stop}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Execution time: 0.720 s
    \end{Verbatim}

    Extract the best model and evaluate it:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} predict labels by best model}
\PY{n}{bestModel} \PY{o}{=} \PY{n}{searchResults}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}

\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{bestModel}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} calculate cross validation score from the best model}
\PY{c+c1}{\PYZsh{} HINT: do NOT use the accuracy score \PYZhy{} it\PYZsq{}s to inaccurate!}
\PY{n}{accuracies} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{bestModel}\PY{p}{,} \PY{n}{X} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{,} 
                             \PY{n}{y} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cross\PYZhy{}validation score: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard Deviation: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracies}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Cross-validation score: 97.50 \%
Standard Deviation: 3.82 \%
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{classification\PYZus{}report}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{classification\PYZus{}report}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                 precision    recall  f1-score   support

    Iris-setosa       1.00      1.00      1.00        13
Iris-versicolor       1.00      0.86      0.92         7
 Iris-virginica       0.91      1.00      0.95        10

       accuracy                           0.97        30
      macro avg       0.97      0.95      0.96        30
   weighted avg       0.97      0.97      0.97        30

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{white}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} print colored confusion matrix}
\PY{n}{cm\PYZus{}colored} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{ConfusionMatrixDisplay}\PY{o}{.}\PY{n}{from\PYZus{}predictions}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Colored Confusion Matrix}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figwidth}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{)}
\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{figure\PYZus{}}\PY{o}{.}\PY{n}{set\PYZus{}figheight}\PY{p}{(}\PY{l+m+mi}{7}\PY{p}{)}

\PY{n}{cm\PYZus{}colored}\PY{o}{.}\PY{n}{confusion\PYZus{}matrix}

\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{figure}
        \begin{center}\includegraphics[scale=0.6]{Step-by-step_intro_to_ML_with_SVC_and_Iris_files/Step-by-step_intro_to_ML_with_SVC_and_Iris_215_0.png}\end{center}
        \caption{Confusion matrix for cross-validation after the randomized search has been performed}
        \label{fig:cm_random_search}
    \end{figure}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{bestModel}\PY{o}{.}\PY{n}{get\PYZus{}params}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\{'C': 10,
 'break\_ties': False,
 'cache\_size': 200,
 'class\_weight': None,
 'coef0': 0.0,
 'decision\_function\_shape': 'ovr',
 'degree': 1,
 'gamma': 0.1,
 'kernel': 'rbf',
 'max\_iter': -1,
 'probability': False,
 'random\_state': None,
 'shrinking': True,
 'tol': 0.001,
 'verbose': False\}
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{summary-and-outlook}{%
\section{Summary and outlook}\label{summary-and-outlook}}

\hypertarget{english-summary}{%
\subsection{English summary}\label{english-summary}}

    \hypertarget{german-summary}{%
\subsection{German summary}\label{german-summary}}

    \hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}


    % Add a bibliography block to the postdoc
    
    
    % Use bibliography
    \printbibheading[heading=bibnumbered]
    \printbibliography[heading=subbibliography,keyword={URL},title={Online references}]
    \printbibliography[heading=subbibliography,keyword={book},title={Books, technical reports and others}]
    %\printbibliography[heading=subbibliography,title={Others}]
    

\end{document}
